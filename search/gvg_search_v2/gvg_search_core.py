"""
gvg_search_core.py
M√≥dulo otimizado para fun√ß√µes de busca principais GvG
Cont√©m apenas as fun√ß√µes de busca realmente utilizadas com IA integrada
"""

import os
import re
import time
import numpy as np
from typing import Dict, List, Tuple, Any, Optional
import psycopg2

# ============================================================================
# NOTA: Este m√≥dulo foi atualizado para replicar as queries e funcionalidades
#       avan√ßadas do antigo gvg_search_utils_v3 (semantic / keyword / hybrid)
#       mantendo compatibilidade com o schema atual (snake_case) e provendo
#       fallbacks seguros quando features (tsvector ou colunas) n√£o existirem.
# ============================================================================

# Importa√ß√µes dos m√≥dulos otimizados
from gvg_database import create_connection
from gvg_ai_utils import get_embedding, get_negation_embedding, calculate_confidence
from gvg_schema import (
	# din√¢micos
	get_main_table, get_emb_table, get_primary_key,
	get_embedding_field, get_embedding_cast, get_embedding_column_expr,
	get_fts_source_field, get_expired_date_field,
	get_core_fields_dict, get_core_columns,
	get_current_source,
	# builders
	build_semantic_select, build_category_similarity_select,
	# est√°ticos √∫teis
	CATEGORIA_TABLE
)

# ============================================================================
# Filtro de Relev√¢ncia interno (removida depend√™ncia de gvg_search_utils_v3)
# Implementa sistema de 3 n√≠veis com uso opcional de OpenAI Assistant.
# N√≠vel 1: desativado | N√≠vel 2: flex√≠vel | N√≠vel 3: restritivo
# ============================================================================
RELEVANCE_FILTER_LEVEL = 1  # 1=sem filtro, 2=flex√≠vel, 3=restritivo
DEBUG_RELEVANCE_FILTER = False
USE_PARTIAL_DESCRIPTION = True

# IDs de assistants de relev√¢ncia (Flex√≠vel e Restritivo)
RELEVANCE_ASSISTANT_FLEXIBLE = os.getenv("GVG_RELEVANCE_FLEXIBLE", "asst_tfD5oQxSgoGhtqdKQHK9UwRi")
RELEVANCE_ASSISTANT_RESTRICTIVE = os.getenv("GVG_RELEVANCE_RESTRICTIVE", "asst_XmsefQEKbuVWu51uNST7kpYT")

try:
	from openai import OpenAI  # type: ignore
	_openai_key = os.getenv("OPENAI_API_KEY")
	if _openai_key:
		_openai_client = OpenAI(api_key=_openai_key)
		_relevance_thread = None
		_RELEVANCE_AVAILABLE = True
	else:
		_openai_client = None
		_relevance_thread = None
		_RELEVANCE_AVAILABLE = False
except Exception:
	_openai_client = None
	_relevance_thread = None
	_RELEVANCE_AVAILABLE = False

def _get_current_assistant_id():
	if RELEVANCE_FILTER_LEVEL == 2:
		return RELEVANCE_ASSISTANT_FLEXIBLE
	if RELEVANCE_FILTER_LEVEL == 3:
		return RELEVANCE_ASSISTANT_RESTRICTIVE
	return None

def _get_relevance_thread():
	global _relevance_thread
	if not _RELEVANCE_AVAILABLE or not _openai_client:
		return None
	if _relevance_thread is None:
		try:
			_relevance_thread = _openai_client.beta.threads.create()
		except Exception as e:
			if DEBUG_RELEVANCE_FILTER:
				print(f"‚ùå Erro criando thread relev√¢ncia: {e}")
			return None
	return _relevance_thread

def _poll_run(thread_id, run_id, timeout=30):
	if not _openai_client:
		raise RuntimeError("Cliente OpenAI indispon√≠vel")
	start = time.time()
	while time.time() - start < timeout:
		run = _openai_client.beta.threads.runs.retrieve(thread_id=thread_id, run_id=run_id)
		if run.status == 'completed':
			msgs = _openai_client.beta.threads.messages.list(thread_id=thread_id, limit=1)
			if msgs.data and msgs.data[0].role == 'assistant':
				content = msgs.data[0].content[0].text.value
				return content
			break
		if run.status in ('failed','cancelled','expired'):
			raise RuntimeError(f"Run falhou: {run.status}")
		time.sleep(1)
	raise TimeoutError("Timeout aguardando resposta do Assistant")

def _call_relevance_assistant(assistant_id, payload_json):
	if not _openai_client:
		raise RuntimeError("Cliente OpenAI indispon√≠vel")
	thread = _get_relevance_thread()
	if not thread:
		raise RuntimeError("Thread de relev√¢ncia indispon√≠vel")
	_openai_client.beta.threads.messages.create(
		thread_id=thread.id,
		role="user",
		content=payload_json
	)
	run = _openai_client.beta.threads.runs.create(
		thread_id=thread.id,
		assistant_id=assistant_id
	)
	return _poll_run(thread.id, run.id, timeout=60)

def _extract_json_block(text: str) -> str:
	if "```json" in text:
		try:
			return text.split("```json",1)[1].split("```",1)[0].strip()
		except Exception:
			pass
	if "```" in text:
		try:
			return text.split("```",1)[1].split("```",1)[0].strip()
		except Exception:
			pass
	return text.strip()

def _prepare_relevance_payload(results, query, search_metadata=None):
	data = []
	for r in results:
		details = r.get('details', {})
		desc = details.get('objeto_compra', '')
		if USE_PARTIAL_DESCRIPTION and '::' in desc:
			desc = desc.split('::')[0].strip()
		data.append({
			'position': r.get('rank', 0) or r.get('details',{}).get('rank',0),
			'description': desc
		})
	return {
		'query': query,
		'search_type': (search_metadata or {}).get('search_type','Desconhecido'),
		'results': data
	}

def _process_relevance_response(response_text, original_results):
	try:
		block = _extract_json_block(response_text)
		import json as _json
		try:
			positions = _json.loads(block)
		except Exception:
			cleaned = block.replace('[','').replace(']','').replace(',',' ')
			positions = [int(x) for x in cleaned.split() if x.isdigit()]
		if not positions:
			return original_results
		pos_set = set(positions)
		filtered = [r for r in original_results if (r.get('rank') in pos_set)]
		order = {p:i for i,p in enumerate(positions)}
		filtered.sort(key=lambda r: order.get(r.get('rank'), 999))
		for i,r in enumerate(filtered,1):
			r['rank'] = i
		return filtered
	except Exception as e:
		if DEBUG_RELEVANCE_FILTER:
			print(f"‚ö†Ô∏è Erro processando resposta relev√¢ncia: {e}")
		return original_results

def apply_relevance_filter(results, query, search_metadata=None):
	if RELEVANCE_FILTER_LEVEL == 1 or not results:
		return results, {'filter_applied': False, 'level': RELEVANCE_FILTER_LEVEL}
	if not _RELEVANCE_AVAILABLE:
		return results, {'filter_applied': False, 'reason':'OpenAI indispon√≠vel','level':RELEVANCE_FILTER_LEVEL}
	assistant_id = _get_current_assistant_id()
	if not assistant_id:
		return results, {'filter_applied': False, 'reason':'Assistant n√£o definido','level':RELEVANCE_FILTER_LEVEL}
	try:
		import json as _json
		payload = _prepare_relevance_payload(results, query, search_metadata)
		if DEBUG_RELEVANCE_FILTER:
			print(f"üîç Relev√¢ncia N√≠vel {RELEVANCE_FILTER_LEVEL} - Enviando {len(payload['results'])} resultados")
		response = _call_relevance_assistant(assistant_id, _json.dumps(payload, ensure_ascii=False))
		filtered = _process_relevance_response(response, results)
		return filtered, {
			'filter_applied': True,
			'original_count': len(results),
			'filtered_count': len(filtered),
			'level': RELEVANCE_FILTER_LEVEL
		}
	except Exception as e:
		if DEBUG_RELEVANCE_FILTER:
			print(f"‚ö†Ô∏è Filtro relev√¢ncia falhou: {e}")
		return results, {'filter_applied': False, 'reason': str(e), 'level': RELEVANCE_FILTER_LEVEL}

def set_relevance_filter_level(level: int):
	global RELEVANCE_FILTER_LEVEL
	if level not in (1,2,3):
		raise ValueError('N√≠vel deve ser 1,2 ou 3')
	RELEVANCE_FILTER_LEVEL = level
	if DEBUG_RELEVANCE_FILTER:
		names={1:'Sem filtro',2:'Flex√≠vel',3:'Restritivo'}
		print(f"üéØ Filtro de Relev√¢ncia: {names[level]} (n√≠vel {level})")

def toggle_relevance_filter(enable: bool=True):
	if enable:
		if RELEVANCE_FILTER_LEVEL==1:
			set_relevance_filter_level(2)
	else:
		set_relevance_filter_level(1)

def get_relevance_filter_status():
	return {
		'level': RELEVANCE_FILTER_LEVEL,
		'enabled': RELEVANCE_FILTER_LEVEL>1,
		'openai_available': _RELEVANCE_AVAILABLE,
		'partial_description': USE_PARTIAL_DESCRIPTION
	}

def toggle_relevance_filter_debug(enable: bool=True):
	global DEBUG_RELEVANCE_FILTER
	DEBUG_RELEVANCE_FILTER = enable
	print(f"üêõ Debug Relev√¢ncia: {'ATIVADO' if enable else 'DESATIVADO'}")

# Configura√ß√µes
MAX_RESULTS = 30
MIN_RESULTS = 5
SEMANTIC_WEIGHT = 0.75
DEFAULT_FILTER_EXPIRED = True
DEFAULT_USE_NEGATION = True

# Flags globais para funcionalidades inteligentes
ENABLE_INTELLIGENT_PROCESSING = True
DEBUG_INTELLIGENT_QUERIES = False

# Utilit√°rio de log unificado (respeita GVG_SQL_DEBUG e DEBUG_INTELLIGENT_QUERIES)
def _sql_debug_enabled():
	try:
		return os.getenv("GVG_SQL_DEBUG", "0") != "0" or DEBUG_INTELLIGENT_QUERIES
	except Exception:
		return DEBUG_INTELLIGENT_QUERIES

def _log(msg: str):
	if _sql_debug_enabled():
		try:
			print(msg)
		except Exception:
			pass


def _safe_close(cursor, conn):
	try:
		if cursor: cursor.close()
	finally:
		if conn: conn.close()

def _get_processed(query_text: str):
	"""Executa processamento inteligente com fallback simples."""
	processed = {
		'original_query': query_text,
		'search_terms': query_text,
		'sql_conditions': [],
		'explanation': 'Processamento n√£o aplicado'
	}
	try:
		if ENABLE_INTELLIGENT_PROCESSING:
			from gvg_preprocessing import SearchQueryProcessor
			processor = SearchQueryProcessor()
			candidate = processor.process_query(query_text)
			if isinstance(candidate, dict):
				aliases = {
					'original_query': ['original_query','raw_query','query_original'],
					'search_terms': ['search_terms','terms','termos_busca'],
					'sql_conditions': ['sql_conditions','conditions','condicoes_sql'],
					'negative_terms': ['negative_terms','negatives','termos_negativos'],
					'explanation': ['explanation','explicacao','explain']
				}
				for target, keys in aliases.items():
					for k in keys:
						if k in candidate and candidate[k] is not None:
							processed[target] = candidate[k]
							break
				if not isinstance(processed.get('sql_conditions'), list):
					processed['sql_conditions'] = []
				if not processed.get('search_terms'):
					processed['search_terms'] = query_text
				if not processed.get('original_query'):
					processed['original_query'] = query_text
				if not processed.get('explanation'):
					processed['explanation'] = 'Processamento aplicado'
			else:
				if DEBUG_INTELLIGENT_QUERIES:
					print(f"‚ö†Ô∏è Preprocessor retorno inesperado: {type(candidate)}")
	except Exception as e:
		if DEBUG_INTELLIGENT_QUERIES:
			print(f"‚ö†Ô∏è Falha preprocessamento: {e}")
	return processed

def _ensure_vector_cast(param_placeholder: str = "%s"):
	"""Retorna express√£o de cast para vetor (compat pgvector)."""
	return f"{param_placeholder}::vector"

# --------------------------------------------------------------
# Sanitiza√ß√£o de condi√ß√µes SQL retornadas pelo Assistant
# - Escapa placeholders "%s" para n√£o conflitar com psycopg2
# - Ajusta c.ano_compra comparando com n√∫meros (ano_compra √© TEXT)
# - BETWEEN/IN com anos num√©ricos -> strings
# - No modo keyword, remove refer√™ncias a ce.* (sem JOIN de embeddings)
# --------------------------------------------------------------
def _sanitize_sql_conditions(sql_conditions, context: str = 'generic'):
	if not isinstance(sql_conditions, (list, tuple)):
		return []
	out = []
	for cond in sql_conditions:
		if not isinstance(cond, str):
			continue
		c = cond
		# Escapar placeholders literais
		if '%s' in c:
			c = c.replace('%s', '%%s')
		# ano_compra compara√ß√µes num√©ricas -> strings
		# ex: c.ano_compra <= 2026  => c.ano_compra <= '2026'
		c = re.sub(r"\bc\.ano_compra\s*(=|<>|!=|<=|>=|<|>)\s*(\d{4})\b", lambda m: f"c.ano_compra {m.group(1)} '{m.group(2)}'", c, flags=re.IGNORECASE)
		# BETWEEN
		c = re.sub(r"\bc\.ano_compra\s+BETWEEN\s+(\d{4})\s+AND\s+(\d{4})\b", lambda m: f"c.ano_compra BETWEEN '{m.group(1)}' AND '{m.group(2)}'", c, flags=re.IGNORECASE)
		# IN (anos)
		def _quote_in_years(match):
			inside = match.group(1)
			nums = re.findall(r"\d{4}", inside)
			if nums:
				quoted = ",".join(f"'{n}'" for n in nums)
				return f"c.ano_compra IN ({quoted})"
			return match.group(0)
		c = re.sub(r"\bc\.ano_compra\s+IN\s*\(([^)]*)\)", _quote_in_years, c, flags=re.IGNORECASE)
		# No keyword mode, evitar ce.*
		if context == 'keyword' and re.search(r"\bce\.", c, flags=re.IGNORECASE):
			continue
		out.append(c)
	return out

# --------------------------------------------------------------
# Compatibilidade: gerar aliases adicionais nos detalhes para que
# c√≥digo legado que procura chaves sem underscore ou variantes
# (ex: modadisputanome) n√£o resulte em N/A.
# --------------------------------------------------------------
_ALIAS_SPECIAL = {
	'modo_disputa_id': ['modadisputaid','modaDisputaId'],
	'modo_disputa_nome': ['modadisputanome','modaDisputaNome'],
	'orgao_entidade_razao_social': ['orgaoentidade_razaosocial','nomeorgaoentidade','orgaoEntidade_razaosocial'],
	'unidade_orgao_nome_unidade': ['unidadeorgao_nomeunidade','unidadeOrgao_nomeUnidade'],
	'unidade_orgao_municipio_nome': ['unidadeorgao_municipionome','unidadeorgao_municipioNome','unidadeOrgao_municipioNome','municipioentidade'],
	'unidade_orgao_uf_sigla': ['unidadeorgao_ufsigla','unidadeOrgao_ufSigla','uf'],
	'objeto_compra': ['objeto','descricaoCompleta'],
	'valor_total_estimado': ['valortotalestimado','valorTotalEstimado'],
	'valor_total_homologado': ['valortotalhomologado','valorTotalHomologado','valorfinal','valorFinal'],
	'data_encerramento_proposta': ['dataencerramentoproposta','dataEncerramentoProposta','dataEncerramento'],
	'data_abertura_proposta': ['dataaberturaproposta','dataAberturaProposta'],
	'modalidade_nome': ['modalidadenome','modalidadeNome'],
	'modalidade_id': ['modalidadeid','modalidadeId'],
	# contrato
	'objeto_contrato': ['objeto','descricaoCompleta'],
	'valor_global': ['valorfinal','valorFinal','valorTotalHomologado'],
	'data_assinatura': ['dataassinatura','dataAssinatura'],
	'data_vigencia_fim': ['dataencerramento','dataEncerramento','dataEncerramentoProposta'],
	'orgao_entidade_razaosocial': ['orgaoentidade_razaosocial','orgaoEntidade_razaosocial','nomeorgaoentidade']
}
def _augment_aliases(d: dict):
	try:
		items = list(d.items())
		for k,v in items:
			if v in (None,''):
				continue
			flat = k.replace('_','')
			if flat not in d:
				d[flat] = v
			if k in _ALIAS_SPECIAL:
				for alt in _ALIAS_SPECIAL[k]:
					if alt not in d:
						d[alt] = v
	except Exception:
		pass
	return d

def semantic_search(query_text,
					limit: int = MAX_RESULTS,
					min_results: int = MIN_RESULTS,
					filter_expired: bool = DEFAULT_FILTER_EXPIRED,
					use_negation: bool = DEFAULT_USE_NEGATION,
					intelligent_mode: bool = True,
					category_codes: Optional[List[str]] = None,
					pre_limit_ids: Optional[int] = None,
					pre_knn_limit: Optional[int] = None):
	"""Busca sem√¢ntica usando builder centralizado de SELECT.

	Agora utiliza `build_semantic_select` para evitar repeti√ß√£o de lista de colunas.
	"""
	conn = None
	cursor = None
	try:
		processed = _get_processed(query_text) if (intelligent_mode and ENABLE_INTELLIGENT_PROCESSING) else {
			'original_query': query_text,
			'search_terms': query_text,
			'negative_terms': '',
			'sql_conditions': [],
			'explanation': 'Processamento desativado'
		}
		if _sql_debug_enabled():
			_log(f"[IN][semantic] query='{query_text}' processed_terms='{processed.get('search_terms','')}' negative_terms='{processed.get('negative_terms','')}' conditions={len(processed.get('sql_conditions',[]))}")
		negative_terms = processed.get('negative_terms') or ''
		search_terms = processed.get('search_terms') or query_text
		embedding_input = f"{search_terms} -- {negative_terms}".strip() if negative_terms else search_terms
		sql_conditions = processed.get('sql_conditions', [])

		emb = get_negation_embedding(embedding_input) if use_negation else get_embedding(embedding_input)
		if emb is None:
			return [], 0.0
		emb_vec = emb.tolist() if isinstance(emb, np.ndarray) else emb
		emb_dim = (len(emb_vec) if isinstance(emb_vec, (list, tuple)) else 'N/A')

		conn = create_connection()
		if not conn:
			return [], 0.0
		cursor = conn.cursor()

		# Ajustes de sess√£o para IVFFLAT nas fontes com halfvec (contrato e contratacao)
		try:
			if get_current_source() in ('contrato','contratacao'):
				try:
					probes = int(os.getenv('IVFFLAT_PROBES', '32'))
				except Exception:
					probes = 32
				# SET LOCAL garante escopo da transa√ß√£o atual
				cursor.execute(f"SET LOCAL ivfflat.probes = {probes}")
				cursor.execute("SET LOCAL enable_seqscan = off")
		except Exception as _sess_err:
			_log(f"‚ö†Ô∏è N√£o foi poss√≠vel ajustar sess√£o IVFFLAT: {_sess_err}")

		vector_opt_enabled = os.getenv("GVG_VECTOR_OPT", "1") != "0"
		# Para 'contrato' e 'contratacao' com halfvec, preferimos kNN direta (evita CTE pr√©-filtro)
		if get_current_source() in ('contrato','contratacao'):
			vector_opt_enabled = False
		executed_optimized = False
		sql_debug = os.getenv("GVG_SQL_DEBUG", "0") != "0"
		sql_conditions_sanitized = _sanitize_sql_conditions(sql_conditions, context='semantic')

		if vector_opt_enabled:
			try:
				core_cols = get_core_columns('c')
				core_cols_expr = ",\n  ".join(core_cols)
				pre_ids = pre_limit_ids if pre_limit_ids is not None else int(os.getenv("GVG_PRE_ID_LIMIT", "50000"))
				pre_knn = pre_knn_limit if pre_knn_limit is not None else int(os.getenv("GVG_PRE_KNN_LIMIT", "5000"))

				main_tbl = get_main_table(); emb_tbl = get_emb_table(); pk = get_primary_key()
				# Dist√¢ncia calculada sobre a express√£o de coluna com cast adequado
				emb_field_name = get_embedding_field()
				col_expr = get_embedding_column_expr('ce')
				emb_cast = get_embedding_cast('%s')
				exp_field = get_expired_date_field()
				_src = get_current_source()

				where_cand = [f"ce.{emb_field_name} IS NOT NULL"]
				if filter_expired:
					where_cand.append(f"to_date(NULLIF(c.{exp_field},''),'YYYY-MM-DD') >= CURRENT_DATE")
				for cond in sql_conditions_sanitized:
					where_cand.append(cond)
				include_categories = bool(category_codes)
				if include_categories:
					where_cand.append("ce.top_categories && %s::text[]")

				cte_parts = [
					"WITH candidatos AS (",
					f"  SELECT ce.{pk}",
					f"  FROM {emb_tbl} ce",
					f"  JOIN {main_tbl} c ON c.{pk} = ce.{pk}",
					"  WHERE " + " AND ".join(where_cand),
					"  LIMIT %s",
					")",
					" , base AS (",
					f"  SELECT ce.{pk}, ({col_expr} <=> {emb_cast}) AS distance",
					f"  FROM {emb_tbl} ce",
					f"  JOIN candidatos x ON x.{pk} = ce.{pk}",
					"  ORDER BY distance ASC",
					"  LIMIT %s",
					")",
					"SELECT",
					f"  {core_cols_expr},",
					"  (1 - base.distance) AS similarity",
					f"FROM base JOIN {main_tbl} c ON c.{pk} = base.{pk}",
					"ORDER BY similarity DESC",
					"LIMIT %s"
				]
				final_sql = "\n".join(cte_parts)

				params: List[Any] = []
				if include_categories:
					params.append(category_codes)
				params.append(pre_ids)
				params.append(emb_vec)
				params.append(pre_knn)
				params.append(limit)

				_log("\n[SQL][semantic_opt]")
				_log(f"source={_src} main={main_tbl} emb={emb_tbl} pk={pk} emb_field={emb_field_name} exp_field={exp_field}")
				_log(final_sql)
				_log(f"params: categories={'Y' if include_categories else 'N'}, pre_ids={pre_ids}, pre_knn={pre_knn}, final_limit={limit}, emb_dim={emb_dim}")

				_t0=time.time()
				cursor.execute(final_sql, params)
				_log(f"[SQL][semantic_opt] executed in {(time.time()-_t0):.3f}s")
				executed_optimized = True
			except Exception as opt_err:
				_log(f"‚ö†Ô∏è Vetor otimizado falhou: {opt_err}")
				try:
					if conn:
						conn.rollback()
				except Exception:
					pass
				executed_optimized = False

		if not executed_optimized:
			base_query = [build_semantic_select('%s').rstrip()]
			params = [emb_vec]
			if category_codes:
				base_query.append("AND ce.top_categories && %s::text[]")
				params.append(category_codes)
			if filter_expired:
				exp_field = get_expired_date_field()
				base_query.append(f"AND to_date(NULLIF(c.{exp_field},''),'YYYY-MM-DD') >= CURRENT_DATE")
			for cond in sql_conditions_sanitized:
				base_query.append(f"AND {cond}")
			# Para 'contrato', ordenar diretamente pelo operador kNN para ativar o IVFFLAT
			if get_current_source() in ('contrato','contratacao'):
				col_expr = get_embedding_column_expr('ce')
				emb_cast = get_embedding_cast('%s')
				base_query.append(f"ORDER BY {col_expr} <=> {emb_cast} ASC")
				base_query.append("LIMIT %s")
				# duplicamos o embedding: um para a coluna similarity (SELECT) e outro para o ORDER BY
				params.append(emb_vec)
				params.append(limit)
			else:
				base_query.append("ORDER BY similarity DESC")
				base_query.append("LIMIT %s")
				params.append(limit)
			final_sql = "\n".join(base_query)
			_log("\n[SQL][semantic_fallback]")
			_log(final_sql)
			_log(f"params: categories={'Y' if bool(category_codes) else 'N'}, final_limit={limit}, emb_dim={emb_dim}")
			_t0=time.time()
			cursor.execute(final_sql, params)
			_log(f"[SQL][semantic_fallback] executed in {(time.time()-_t0):.3f}s")

		rows = cursor.fetchall()
		column_names = [d[0] for d in cursor.description]

		results: List[Dict[str, Any]] = []
		core_keys = set(get_core_fields_dict().keys())
		for idx, row in enumerate(rows):
			record = dict(zip(column_names, row))
			similarity = float(record['similarity'])
			details = {k: v for k, v in record.items() if k in core_keys}
			details['similarity'] = similarity
			if intelligent_mode:
				details['intelligent_processing'] = {
					'original_query': processed.get('original_query', query_text),
					'processed_terms': processed.get('search_terms', query_text),
					'applied_conditions': len(sql_conditions),
					'explanation': processed.get('explanation','')
				}
			_augment_aliases(details)
			pk_cur = get_primary_key()
			results.append({
				'id': record.get(pk_cur),
				'numero_controle': record.get(pk_cur),
				'similarity': similarity,
				'rank': idx + 1,
				'details': details
			})

		# Logs de sa√≠da
		try:
			if _sql_debug_enabled():
				sims = [float(r.get('similarity',0)) for r in results]
				if sims:
					_log(f"[OUT][semantic] rows={len(results)} sim_min={min(sims):.4f} sim_max={max(sims):.4f} sim_avg={(sum(sims)/len(sims)):.4f}")
				else:
					_log("[OUT][semantic] rows=0")
		except Exception:
			pass

		if RELEVANCE_FILTER_LEVEL > 1 and results:
			meta = {
				'search_type': 'Sem√¢ntica' + (' (Inteligente)' if intelligent_mode else ''),
				'search_approach': 'Direta',
				'sort_mode': 'Similaridade'
			}
			try:
				filtered, _filter_info = apply_relevance_filter(results, query_text, meta)
				if filtered:
					results = filtered
			except Exception as rf_err:
				if DEBUG_INTELLIGENT_QUERIES:
					print(f"‚ö†Ô∏è Filtro de relev√¢ncia falhou: {rf_err}")

		return results, calculate_confidence([r['similarity'] for r in results])
	except Exception as e:
		try:
			if conn:
				conn.rollback()
		except Exception:
			pass
		if os.getenv("GVG_SQL_DEBUG", "0") != "0" or DEBUG_INTELLIGENT_QUERIES:
			print(f"[ERRO][semantic_search] {type(e).__name__}: {e}")
		print(f"Erro na busca sem√¢ntica: {e}")
		return [], 0.0
	finally:
		_safe_close(cursor, conn)

def keyword_search(query_text, limit=MAX_RESULTS, min_results=MIN_RESULTS,
				   filter_expired=DEFAULT_FILTER_EXPIRED,
				   intelligent_mode=True):
	"""Busca por palavras‚Äëchave usando full‚Äëtext search.

	Usa builders para colunas core e normaliza uma m√©trica de similaridade
	baseada nos ranks retornados pelo PostgreSQL.
	"""
	conn = None; cursor = None
	try:
		if intelligent_mode and ENABLE_INTELLIGENT_PROCESSING:
			processed = _get_processed(query_text)
		else:
			processed = {
				'original_query': query_text,
				'search_terms': query_text,
				'negative_terms': '',
				'sql_conditions': [],
				'explanation': 'Processamento desativado'
			}
		if _sql_debug_enabled():
			_log(f"[IN][keyword] query='{query_text}' processed_terms='{processed.get('search_terms','')}' negative_terms='{processed.get('negative_terms','')}' conditions={len(processed.get('sql_conditions',[]))}")
		search_terms = (processed.get('search_terms') or query_text).strip()
		negative_terms = (processed.get('negative_terms') or '').strip()
		sql_conditions = processed.get('sql_conditions', [])
		sql_conditions_sanitized = _sanitize_sql_conditions(sql_conditions, context='keyword')

		conn = create_connection()
		if not conn:
			return [], 0.0
		cursor = conn.cursor()

		terms_split = [t for t in search_terms.split() if t]
		if not terms_split:
			return [], 0.0
		tsquery = ' & '.join(terms_split)
		tsquery_prefix = ':* & '.join(terms_split) + ':*'

		# Parse negative terms -> tokens alfanum√©ricos √∫nicos
		neg_tokens = []
		if negative_terms:
			raw_tokens = re.findall(r"[\w√Ä-√ø]+", negative_terms.lower())
			# remover duplicados preservando ordem
			seen = set()
			for t in raw_tokens:
				if t and t not in seen:
					seen.add(t)
					neg_tokens.append(t)

		core_cols = get_core_columns('c')
		text_field = f"c.{get_fts_source_field()}"
		base = [
			"SELECT",
			"  " + ",\n  ".join(core_cols) + ",",
			# rank principal (exato)
			f"  ts_rank(to_tsvector('portuguese', {text_field}), to_tsquery('portuguese', %s)) AS rank_exact,",
			# rank auxiliar (prefixo) com peso menor
			f"  ts_rank(to_tsvector('portuguese', {text_field}), to_tsquery('portuguese', %s)) AS rank_prefix",
			f"FROM {get_main_table()} c",
			"WHERE (",
			f"  to_tsvector('portuguese', {text_field}) @@ to_tsquery('portuguese', %s)",
			f"  OR to_tsvector('portuguese', {text_field}) @@ to_tsquery('portuguese', %s)",
			")"
		]
		# Exclus√µes por termos negativos (prefix match) via NOT @@ (OR de negativos)
		if neg_tokens:
			neg_query = ' | '.join(f"{t}:*" for t in neg_tokens)
			base.append("AND NOT (to_tsvector('portuguese', {tf}) @@ to_tsquery('portuguese', %s))".format(tf=text_field))
		if filter_expired:
			exp_field = get_expired_date_field()
			base.append(f"AND to_date(NULLIF(c.{exp_field},''),'YYYY-MM-DD') >= CURRENT_DATE")
		for cond in sql_conditions_sanitized:
			base.append(f"AND {cond}")
		# Ordena√ß√£o simplificada: combina√ß√£o linear j√° calculada em Python; aqui priorizamos exato depois prefixo
		base.append("ORDER BY rank_exact DESC, rank_prefix DESC")
		base.append("LIMIT %s")
		sql = "\n".join(base)
		params = [tsquery, tsquery_prefix, tsquery, tsquery_prefix]
		if neg_tokens:
			params.append(neg_query)
		params.append(limit)
		sql_debug = os.getenv("GVG_SQL_DEBUG", "0") != "0"
		sql_debug = os.getenv("GVG_SQL_DEBUG", "0") != "0"
		if sql_debug or DEBUG_INTELLIGENT_QUERIES:
			_log("\n[SQL][keyword]")
			_log(f"source={get_current_source()} text_field={text_field}")
			_log(sql)
			_log(f"params={params}")
		_t0=time.time()
		cursor.execute(sql, params)
		_log(f"[SQL][keyword] executed in {(time.time()-_t0):.3f}s")
		rows = cursor.fetchall()
		column_names = [d[0] for d in cursor.description]
		core_keys = set(get_core_fields_dict().keys())
		results = []
		sims = []
		# Normaliza√ß√£o simples: similarity = min( (rank_exact + 0.5*rank_prefix) / (0.1 * n_terms + 1e-6), 1.0 )
		denom = (0.1 * len(terms_split)) + 1e-6
		for i, row in enumerate(rows):
			rec = dict(zip(column_names, row))
			rank_exact = float(rec['rank_exact'])
			rank_prefix = float(rec['rank_prefix'])
			combined = rank_exact + 0.5 * rank_prefix
			similarity = combined / denom
			if similarity > 1.0:
				similarity = 1.0
			sims.append(similarity)
			details = {k: v for k, v in rec.items() if k in core_keys}
			details['rank_exact'] = rank_exact
			details['rank_prefix'] = rank_prefix
			details['search_terms'] = search_terms
			if negative_terms:
				details['negative_terms'] = negative_terms
			if intelligent_mode:
				details['intelligent_processing'] = {
					'original_query': processed.get('original_query', query_text),
					'processed_terms': processed['search_terms'],
					'applied_conditions': len(sql_conditions),
					'explanation': processed.get('explanation', '')
				}
			_augment_aliases(details)
			pk_cur = get_primary_key()
			results.append({
				'id': rec.get(pk_cur),
				'numero_controle': rec.get(pk_cur),
				'similarity': similarity,
				'rank': i + 1,
				'details': details
			})
		# Logs de sa√≠da
		try:
			if _sql_debug_enabled():
				sims2 = [float(r.get('similarity',0)) for r in results]
				if sims2:
					_log(f"[OUT][keyword] rows={len(results)} sim_min={min(sims2):.4f} sim_max={max(sims2):.4f} sim_avg={(sum(sims2)/len(sims2)):.4f}")
				else:
					_log("[OUT][keyword] rows=0")
		except Exception:
			pass

		if apply_relevance_filter and RELEVANCE_FILTER_LEVEL > 1 and results:
			meta = {
				'search_type': 'Palavras‚Äëchave' + (' (Inteligente)' if intelligent_mode else ''),
				'search_approach': 'Direta',
				'sort_mode': 'Relev√¢ncia'
			}
			try:
				filtered, _ = apply_relevance_filter(results, query_text, meta)
				if filtered:
					results = filtered
			except Exception as rf_err:
				if DEBUG_INTELLIGENT_QUERIES:
					print(f"‚ö†Ô∏è Filtro de relev√¢ncia falhou: {rf_err}")
		return results, calculate_confidence([r['similarity'] for r in results])
	except Exception as e:
		print(f"Erro na busca por palavras‚Äëchave: {e}")
		return [], 0.0
	finally:
		_safe_close(cursor, conn)

def hybrid_search(query_text, limit=MAX_RESULTS, min_results=MIN_RESULTS,
				  semantic_weight=SEMANTIC_WEIGHT,
				  filter_expired=DEFAULT_FILTER_EXPIRED,
				  use_negation=DEFAULT_USE_NEGATION,
				  intelligent_mode=True):
	"""Busca h√≠brida com elimina√ß√£o de hardcodes de colunas.

	Usa builders para colunas core e cursor.description para mapear resultados.
	"""
	conn=None; cursor=None
	try:
		if intelligent_mode and ENABLE_INTELLIGENT_PROCESSING:
			processed = _get_processed(query_text)
		else:
			processed = {
				'original_query': query_text,
				'search_terms': query_text,
				'negative_terms': '',
				'sql_conditions': [],
				'explanation': 'Processamento desativado'
			}
		if _sql_debug_enabled():
			_log(f"[IN][hybrid] query='{query_text}' processed_terms='{processed.get('search_terms','')}' negative_terms='{processed.get('negative_terms','')}' conditions={len(processed.get('sql_conditions',[]))}")
		negative_terms = processed.get('negative_terms') or ''
		search_terms = processed.get('search_terms') or query_text
		embedding_input = f"{search_terms} -- {negative_terms}".strip() if negative_terms else search_terms
		sql_conditions = processed.get('sql_conditions', [])
		sql_conditions_sanitized = _sanitize_sql_conditions(sql_conditions, context='hybrid')

		# Embedding com suporte avan√ßado a nega√ß√£o (n√£o depende apenas de '--')
		if use_negation:
			emb = get_negation_embedding(embedding_input)
		else:
			emb = get_embedding(embedding_input)
		if emb is None:
			return [], 0.0
		if isinstance(emb, np.ndarray):
			emb_vec = emb.tolist()
		else:
			emb_vec = emb

		terms_split = [t for t in search_terms.split() if t]
		tsquery = ' & '.join(terms_split) if terms_split else search_terms
		tsquery_prefix = ':* & '.join(terms_split) + ':*' if terms_split else search_terms
		max_possible_keyword_score = max(len(terms_split)*0.1, 0.0001)

		conn = create_connection()
		if not conn:
			return [], 0.0
		cursor = conn.cursor()

		core_cols = get_core_columns('c')  # din√¢mico
		text_field = f"c.{get_fts_source_field()}"
		emb_field = get_embedding_field()
		emb_cast = get_embedding_cast('%s')
		col_expr = get_embedding_column_expr('ce')
		base = [
			"SELECT",
			"  " + ",\n  ".join(core_cols) + ",",
			f"  (1 - ({col_expr} <=> {emb_cast})) AS semantic_score,",
			f"  COALESCE(ts_rank(to_tsvector('portuguese', {text_field}), to_tsquery('portuguese', %s)),0) AS keyword_score,",
			f"  COALESCE(ts_rank(to_tsvector('portuguese', {text_field}), to_tsquery('portuguese', %s)),0) AS keyword_prefix_score,",
			f"  ( %s * (1 - ({col_expr} <=> {emb_cast})) + (1 - %s) * LEAST((0.7 * COALESCE(ts_rank(to_tsvector('portuguese', {text_field}), to_tsquery('portuguese', %s)) + 0.3 * COALESCE(ts_rank(to_tsvector('portuguese', {text_field}), to_tsquery('portuguese', %s))) ) / %s, 1.0) ) AS combined_score",
			f"FROM {get_main_table()} c",
			f"JOIN {get_emb_table()} ce ON c.{get_primary_key()} = ce.{get_primary_key()}",
			f"WHERE ce.{emb_field} IS NOT NULL"
		]
		if filter_expired:
			exp_field = get_expired_date_field()
			base.append(f"AND to_date(NULLIF(c.{exp_field},''),'YYYY-MM-DD') >= CURRENT_DATE")
		for cond in sql_conditions_sanitized:
			base.append(f"AND {cond}")
		base.append("ORDER BY combined_score DESC")
		base.append("LIMIT %s")
		sql = "\n".join(base)
		params = [emb_vec, tsquery, tsquery_prefix, semantic_weight, emb_vec, semantic_weight, tsquery, tsquery_prefix, max_possible_keyword_score, limit]
		try:
			_log("\n[SQL][hybrid]")
			_log(f"source={get_current_source()} text_field={text_field} emb_field={emb_field}")
			_log(sql)
			_log(f"params: emb_dim={(len(emb_vec) if isinstance(emb_vec,(list,tuple)) else 'N/A')}, tsquery='{tsquery}', limit={limit}, semantic_weight={semantic_weight}")
			_t0=time.time()
			cursor.execute(sql, params)
			_log(f"[SQL][hybrid] executed in {(time.time()-_t0):.3f}s")
			rows = cursor.fetchall(); column_names=[d[0] for d in cursor.description]; results=[]; sims=[]; core_keys=set(get_core_fields_dict().keys())
			for idx,row in enumerate(rows):
				rec=dict(zip(column_names,row))
				combined=float(rec['combined_score'])
				sims.append(combined)
				details={k:v for k,v in rec.items() if k in core_keys}
				# anexa m√©tricas
				details['semantic_score']=float(rec['semantic_score'])
				details['keyword_score']=float(rec['keyword_score'])
				details['keyword_prefix_score']=float(rec['keyword_prefix_score'])
				if intelligent_mode:
					details['intelligent_processing']={
						'original_query': processed.get('original_query', query_text),
						'processed_terms': processed['search_terms'],
						'applied_conditions': len(sql_conditions),
						'explanation': processed.get('explanation','')
					}
				_augment_aliases(details)
				pk_cur = get_primary_key()
				results.append({
					'id': rec.get(pk_cur),
					'numero_controle': rec.get(pk_cur),
					'similarity': combined,
					'rank': idx+1,
					'details': details
				})
			# Logs de sa√≠da
			try:
				if _sql_debug_enabled():
					sims = [float(r.get('similarity',0)) for r in results]
					if sims:
						_log(f"[OUT][hybrid] rows={len(results)} sim_min={min(sims):.4f} sim_max={max(sims):.4f} sim_avg={(sum(sims)/len(sims)):.4f}")
					else:
						_log("[OUT][hybrid] rows=0")
			except Exception:
				pass

			if apply_relevance_filter and RELEVANCE_FILTER_LEVEL > 1 and results:
				meta = {
					'search_type': 'H√≠brida' + (' (Inteligente)' if intelligent_mode else ''),
					'search_approach': 'Direta',
					'sort_mode': 'H√≠brida'
				}
				try:
					filtered, _ = apply_relevance_filter(results, query_text, meta)
					if filtered:
						results = filtered
				except Exception as rf_err:
					if DEBUG_INTELLIGENT_QUERIES:
						print(f"‚ö†Ô∏è Filtro de relev√¢ncia falhou: {rf_err}")
			return results, calculate_confidence([r['similarity'] for r in results])
		except Exception as fe:
			if DEBUG_INTELLIGENT_QUERIES:
				print(f"‚ö†Ô∏è H√≠brida SQL √∫nica falhou, fallback dupla: {fe}")
			# Fallback: combinar duas buscas
			sem_results, sem_conf = semantic_search(query_text, limit, min_results, filter_expired, use_negation, intelligent_mode)
			kw_results, kw_conf = keyword_search(query_text, limit, min_results, filter_expired, intelligent_mode)
			combined = {}
			for r in sem_results:
				combined[r['numero_controle']] = {
					**r,
					'semantic_similarity': r['similarity'],
					'keyword_similarity': 0.0,
					'similarity': r['similarity'] * semantic_weight
				}
			kw_weight = 1 - semantic_weight
			for r in kw_results:
				if r['numero_controle'] in combined:
					combined[r['numero_controle']]['similarity'] += r['similarity'] * kw_weight
					combined[r['numero_controle']]['keyword_similarity'] = r['similarity']
				else:
					combined[r['numero_controle']] = {
						**r,
						'semantic_similarity': 0.0,
						'keyword_similarity': r['similarity'],
						'similarity': r['similarity'] * kw_weight
					}
			final = list(combined.values())
			final.sort(key=lambda x: x['similarity'], reverse=True)
			final = final[:limit]
			# Adicionar rank e aplicar filtro de relev√¢ncia se dispon√≠vel
			for idx, r in enumerate(final):
				r['rank'] = idx + 1
				if 'details' in r:
					_augment_aliases(r['details'])
			if apply_relevance_filter and RELEVANCE_FILTER_LEVEL > 1 and final:
				meta = {
					'search_type': 'H√≠brida (Fallback)',
					'search_approach': 'Fus√£o',
					'sort_mode': 'H√≠brida'
				}
				try:
					filtered, _ = apply_relevance_filter(final, query_text, meta)
					if filtered:
						final = filtered
				except Exception as rf_err:
					if DEBUG_INTELLIGENT_QUERIES:
						print(f"‚ö†Ô∏è Filtro de relev√¢ncia falhou: {rf_err}")
			conf = sem_conf*semantic_weight + kw_conf*kw_weight
			return final, conf
	except Exception as e:
		print(f"Erro na busca h√≠brida: {e}")
		return [], 0.0
	finally:
		_safe_close(cursor, conn)

def toggle_intelligent_processing(enable: bool = True):
	"""
	Ativa/desativa o processamento inteligente de queries
    
	Args:
		enable (bool): True para ativar, False para desativar
	"""
	global ENABLE_INTELLIGENT_PROCESSING
	ENABLE_INTELLIGENT_PROCESSING = enable
	status = "ATIVADO" if enable else "DESATIVADO"
	print(f"üß† Processamento Inteligente: {status}")

def toggle_intelligent_debug(enable: bool = True):
	"""
	Ativa/desativa debug das queries inteligentes
    
	Args:
		enable (bool): True para ativar, False para desativar
	"""
	global DEBUG_INTELLIGENT_QUERIES
	DEBUG_INTELLIGENT_QUERIES = enable
	status = "ATIVADO" if enable else "DESATIVADO" 
	print(f"üêõ Debug Inteligente: {status}")

def get_intelligent_status():
	"""
	Retorna status atual do sistema inteligente
    
	Returns:
		dict: Status das funcionalidades inteligentes
	"""
	return {
		'intelligent_processing': ENABLE_INTELLIGENT_PROCESSING,
		'debug_mode': DEBUG_INTELLIGENT_QUERIES,
		'status': 'ATIVO' if ENABLE_INTELLIGENT_PROCESSING else 'INATIVO'
	}

# =============================================================
# CATEGORIAS (migrado de gvg_categories.py para consolida√ß√£o v3)
# =============================================================
from gvg_database import create_engine_connection
import pandas as pd

def get_top_categories_for_query(query_text: str, top_n: int = 10, use_negation: bool = True, search_type: int = 1, console=None):
	"""Retorna top categorias similares ao embedding da consulta.

	Migrado para usar schema unificado (tabela categoria e campos snake_case).
	Mant√©m formato de sa√≠da legado (keys codigo, descricao, nivelX_) consumido pelo pipeline/UX.
	"""
	try:
		emb = get_embedding(query_text)
		if emb is None:
			return []
		emb_list = emb.tolist() if isinstance(emb, np.ndarray) else emb
		engine = create_engine_connection()
		if not engine:
			return []
		# Usa builder centralizado para garantir consist√™ncia de colunas
		base_select = build_category_similarity_select('%(embedding)s')  # j√° inclui FROM/WHERE
		query = base_select + "ORDER BY similarity DESC LIMIT %(limit)s"  # adiciona ordena√ß√£o/limite
		df = pd.read_sql_query(query, engine, params={"embedding": emb_list, "limit": top_n})
		out = []
		for idx, row in df.iterrows():
			# Compat: alguns ambientes podem n√£o ter id_categoria; usar fallback
			categoria_id = row.get('id_categoria') if 'id_categoria' in df.columns else row.get('cod_cat')
			out.append({
				'rank': idx + 1,
				'categoria_id': categoria_id,
				'codigo': row.get('cod_cat'),
				'descricao': row.get('nom_cat'),
				'nivel0_cod': row.get('cod_nv0'),
				'nivel0_nome': row.get('nom_nv0'),
				'nivel1_cod': row.get('cod_nv1'),
				'nivel1_nome': row.get('nom_nv1'),
				'nivel2_cod': row.get('cod_nv2'),
				'nivel2_nome': row.get('nom_nv2'),
				'nivel3_cod': row.get('cod_nv3'),
				'nivel3_nome': row.get('nom_nv3'),
				'similarity_score': float(row.get('similarity', 0.0))
			})
		return out
	except Exception as e:
		if console:
			console.print(f"[red]Erro ao buscar categorias: {e}[/red]")
		return []

def _calculate_correspondence_similarity_score(query_categories, result_categories, result_similarities):
	try:
		if not query_categories or not result_categories or not result_similarities:
			return 0.0
		best=0.0
		for qc in query_categories:
			code = qc.get('codigo'); qsim = qc.get('similarity_score',0) or 0
			if not code or not qsim: continue
			if code in result_categories:
				idx = result_categories.index(code)
				rsim = result_similarities[idx] if idx < len(result_similarities) else 0
				if rsim:
					best = max(best, float(qsim)*float(rsim))
		return float(best)
	except Exception:
		return 0.0

def _find_top_category_for_result(query_categories, result_categories, result_similarities):
	try:
		if not query_categories or not result_categories or not result_similarities:
			return None
		best_cat=None; best_score=0.0
		for qc in query_categories:
			code=qc.get('codigo'); qsim=qc.get('similarity_score',0) or 0
			if not code or not qsim: continue
			if code in result_categories:
				idx = result_categories.index(code)
				rsim = result_similarities[idx] if idx < len(result_similarities) else 0
				if not rsim: continue
				score = float(qsim)*float(rsim)
				if score>best_score:
					best_score=score
					best_cat={'codigo':code,'descricao':qc.get('descricao'),'query_similarity':qsim,'result_similarity':rsim,'correspondence_score':score}
		return best_cat
	except Exception:
		return None

def correspondence_search(query_text, top_categories, limit=30, filter_expired=True, console=None):
	"""Busca por correspond√™ncia de categorias.

	Atualizada para usar somente tabelas/colunas V1 (contratacao / contratacao_emb).
	"""
	if not top_categories:
		return [], 0.0, {'reason': 'no_categories'}
	try:
		category_codes = [c['codigo'] for c in top_categories if c.get('codigo')]
		if not category_codes:
			return [], 0.0, {'reason': 'empty_codes'}
		conn = create_connection()
		if not conn:
			return [], 0.0, {'reason': 'db_connection_failed'}
		cur = conn.cursor()
		main_tbl = get_main_table(); emb_tbl = get_emb_table(); pk = get_primary_key()
		cols = ",\n\t\t       ".join(get_core_columns('c'))
		sql = f"""
		SELECT {cols},
		       ce.top_categories, ce.top_similarities
		FROM {main_tbl} c
		JOIN {emb_tbl} ce ON c.{pk} = ce.{pk}
		WHERE ce.top_categories && %s
		"""
		params = (category_codes,)
		if filter_expired:
			exp_field = get_expired_date_field()
			sql += f" AND to_date(NULLIF(c.{exp_field},''),'YYYY-MM-DD') >= CURRENT_DATE"
		sql += " LIMIT %s"
		params = (category_codes, limit * 5)
		cur.execute(sql, params)
		rows = cur.fetchall(); colnames = [d[0] for d in cur.description]; cur.close(); conn.close()
		results = []
		for row in rows:
			rec = dict(zip(colnames, row))
			_augment_aliases(rec)
			r_categories = rec.get('top_categories') or []
			r_sims = rec.get('top_similarities') or []
			correspondence_similarity = _calculate_correspondence_similarity_score(top_categories, r_categories, r_sims)
			top_cat_info = _find_top_category_for_result(top_categories, r_categories, r_sims)
			results.append({
				'id': rec.get('numero_controle_pncp'),
				'numero_controle': rec.get('numero_controle_pncp'),
				'similarity': correspondence_similarity,
				'correspondence_similarity': correspondence_similarity,
				'search_approach': 'correspondence',
				'details': rec,
				'top_category_info': top_cat_info
			})
		results.sort(key=lambda x: x['similarity'], reverse=True)
		results = results[:limit]
		for i, r in enumerate(results, 1):
			r['rank'] = i
		confidence = calculate_confidence([r['similarity'] for r in results]) if results else 0.0
		return results, confidence, {'total_raw': len(rows)}
	except Exception as e:
		if console: console.print(f"[red]Erro correspond√™ncia: {e}[/red]")
		return [], 0.0, {'error': str(e)}

def category_filtered_search(query_text, search_type, top_categories, limit=30, filter_expired=True, use_negation=True, expanded_factor=3, console=None):
	"""Filtra resultados por interse√ß√£o com categorias top do usu√°rio.

	Atualizado para apenas schema V1. Mant√©m forma de sa√≠da.
	"""
	if not top_categories:
		return [], 0.0, {'reason': 'no_categories'}
	try:
		expanded_limit = limit * expanded_factor
		if search_type == 1:
			# Extrai c√≥digos e injeta diretamente no SQL sem√¢ntico para filtrar por categorias
			category_codes = [c['codigo'] for c in top_categories if c.get('codigo')]
			base_results, confidence = semantic_search(
				query_text,
				limit=limit,  # j√° limitado no SQL final
				filter_expired=filter_expired,
				use_negation=use_negation,
				intelligent_mode=True,
				category_codes=category_codes
			)
			# Resultados j√° v√™m filtrados por categoria no SQL ‚Äì retornar direto
			if base_results:
				for i, r in enumerate(base_results, 1):
					r['rank'] = i
				meta = {
					'universe_size': len(base_results),
					'universe_with_categories': len(base_results),
					'filtered_count': len(base_results),
					'filtered_by_sql': True
				}
				return base_results, confidence, meta
		elif search_type == 2:
			base_results, confidence = keyword_search(query_text, limit=expanded_limit, filter_expired=filter_expired)
		else:
			base_results, confidence = hybrid_search(query_text, limit=expanded_limit, filter_expired=filter_expired, use_negation=use_negation)
		if not base_results:
			return [], 0.0, {'reason': 'no_base_results'}
		ids = [r['id'] for r in base_results]
		conn = create_connection()
		if not conn:
			return [], 0.0, {'reason': 'db_connection_failed'}
		cur = conn.cursor(); placeholders = ','.join(['%s'] * len(ids))
		pk = get_primary_key(); emb_tbl = get_emb_table()
		cat_sql = f"""
		SELECT {pk}, top_categories
		FROM {emb_tbl}
		WHERE {pk} IN ({placeholders}) AND top_categories IS NOT NULL
		"""
		cur.execute(cat_sql, ids)
		cat_map = {row[0]: row[1] for row in cur.fetchall()}; cur.close(); conn.close()
		query_codes = {c['codigo'] for c in top_categories if c.get('codigo')}
		filtered = []; universe_with_categories = 0
		for r in base_results:
			r_cats = cat_map.get(r['id'])
			if r_cats:
				universe_with_categories += 1
				if any(code in r_cats for code in query_codes):
					r['search_approach'] = 'category_filtered'
					if 'details' in r:
						_augment_aliases(r['details'])
					filtered.append(r)
			if len(filtered) >= limit:
				break
		for i, r in enumerate(filtered, 1):
			r['rank'] = i
		meta = {
			'universe_size': len(base_results),
			'universe_with_categories': universe_with_categories,
			'filtered_count': len(filtered)
		}
		return filtered, confidence, meta
	except Exception as e:
		if console: console.print(f"[red]Erro filtro categorias: {e}[/red]")
		return [], 0.0, {'error': str(e)}

__all__ = [
	'semantic_search','keyword_search','hybrid_search',
	'apply_relevance_filter','set_relevance_filter_level','toggle_relevance_filter','get_relevance_filter_status','toggle_relevance_filter_debug',
	'toggle_intelligent_processing','toggle_intelligent_debug','get_intelligent_status',
	'get_top_categories_for_query','correspondence_search','category_filtered_search'
]
