# =======================================================================
# [5C/7] CATEGORIZA√á√ÉO AUTOM√ÅTICA ULTRA OTIMIZADA - VERS√ÉO V2 (BATCH PGVECTOR)
# =======================================================================
# Vers√£o ultra otimizada do 05B com implementa√ß√£o de BATCH PGVECTOR SEARCH
# e estrutura de query otimizada, mantendo filtro de data com DATE()
# 
# PRINCIPAIS OTIMIZA√á√ïES V5C:
# - Pool de conex√µes thread-safe (ThreadedConnectionPool)
# - Timeouts diferenciados para SELECTs e UPDATEs
# - BATCH PGVECTOR SEARCH: 1 query para m√∫ltiplos embeddings (99% mais r√°pido)
# - Estrutura de query otimizada com pr√©-filtro
# - Batch processing para m√∫ltiplos UPDATEs
# - Context managers robustos com retry autom√°tico
# - Configura√ß√µes de keepalive para conex√µes longas
# - Processamento paralelo otimizado
# 
# OTIMIZA√á√ïES IMPLEMENTADAS:
# ‚úÖ Otimiza√ß√£o 2: Batch pgvector search (1 query vs N queries)
# ‚úÖ Otimiza√ß√£o 3: Estrutura de query otimizada
# ‚ö†Ô∏è  Otimiza√ß√£o 1: Mantido DATE() conforme solicitado pelo usu√°rio
# 
# Resultado: Performance drasticamente melhorada para processamento de similaridade
# =======================================================================

import os
import sys
import time
import math
import threading
import argparse
import logging
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor
from contextlib import contextmanager
import psycopg2
from psycopg2 import pool
from psycopg2.extras import RealDictCursor, execute_values
import numpy as np
from dotenv import load_dotenv
from rich.console import Console
from rich.progress import Progress, TextColumn, BarColumn, SpinnerColumn, TaskProgressColumn, TimeElapsedColumn
from rich.panel import Panel

# Configura√ß√£o global
debug_mode = False
logger = None
connection_pool = None

# Configure Rich console
console = Console()

# Configura√ß√µes de categoriza√ß√£o ULTRA OTIMIZADAS V5C
TOP_K = 5
MIN_WORKERS = 3
MAX_WORKERS = 12   # Otimizado para categoriza√ß√£o (menos que embedding)
BATCH_SIZE = 50    # Batch menor para updates mais frequentes
UPDATE_BATCH_SIZE = 100  # Para batch updates
NUM_CONN = MAX_WORKERS + 4

# NOVA CONFIGURA√á√ÉO V5C: Batch size para pgvector search
PGVECTOR_BATCH_SIZE = 100  # Processa 100 embeddings por vez no batch pgvector

#############################################
# SETUP DE LOGGING E CONFIGURA√á√ïES
#############################################

def parse_arguments():
    """Parse argumentos da linha de comando"""
    parser = argparse.ArgumentParser(description='Categoriza√ß√£o Autom√°tica Ultra Otimizada v2 - Batch PGVector')
    parser.add_argument('--test', 
                       action='store_true',
                       help='Executar para pr√≥ximo dia ap√≥s last_categorization_date')
    parser.add_argument('--debug', 
                       action='store_true',
                       help='Ativar modo debug com logs detalhados')
    return parser.parse_args()

def setup_logging(debug_mode=False):
    """Configura sistema de logging"""
    global logger
    
    # Gerar timestamp para o nome do arquivo de log
    start_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Criar pasta de logs se n√£o existir
    logs_dir = os.path.join(os.path.dirname(script_dir), "logs")
    os.makedirs(logs_dir, exist_ok=True)
    
    LOG_FILE = os.path.join(logs_dir, f"05C_categorization_ultra_batch_pgvector_{start_timestamp}.log")
    
    # Configurar logger principal
    logger = logging.getLogger('categorization_ultra_batch')
    logger.setLevel(logging.DEBUG)
    
    # Formatter para o arquivo de log
    file_formatter = logging.Formatter(
        '%(asctime)s | %(levelname)-8s | %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # Handler para arquivo (sempre ativo)
    file_handler = logging.FileHandler(LOG_FILE, encoding='utf-8')
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(file_formatter)
    logger.addHandler(file_handler)
    
    logger.info("=" * 80)
    logger.info("NOVA SESS√ÉO DE CATEGORIZA√á√ÉO ULTRA OTIMIZADA V5C - BATCH PGVECTOR INICIADA")
    logger.info("=" * 80)
    logger.info(f"Hor√°rio de in√≠cio: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"Modo debug: {'Ativado' if debug_mode else 'Desativado'}")
    logger.info(f"Arquivo de log: {LOG_FILE}")
    logger.info(f"OTIMIZA√á√ïES V5C: Workers={MAX_WORKERS}, BatchSize={BATCH_SIZE}, Pool={NUM_CONN} conex√µes, PGVector Batch={PGVECTOR_BATCH_SIZE}")
    logger.info("-" * 80)
    
    return logger, os.path.basename(LOG_FILE)

def log_session_end(stats, tempo_total):
    """Finaliza a sess√£o de log com resumo"""
    if logger:
        logger.info("-" * 80)
        logger.info("SESS√ÉO DE CATEGORIZA√á√ÉO ULTRA OTIMIZADA V5C - BATCH PGVECTOR FINALIZADA")
        logger.info(f"Hor√°rio de t√©rmino: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"Tempo total: {tempo_total:.1f}s")
        logger.info(f"Datas processadas: {stats['datas_processadas']}")
        logger.info(f"Embeddings processados: {stats['embeddings_processados']:,}")
        logger.info(f"Categoriza√ß√µes aplicadas: {stats['categorizacoes_aplicadas']:,}")
        logger.info(f"Erros encontrados: {stats['erros']:,}")
        logger.info("OTIMIZA√á√ïES V5C: Pool conex√µes, batch pgvector search, timeouts otimizados, batch updates")
        logger.info("=" * 80)

def format_date_for_display(date_str):
    """Converte YYYYMMDD para DD/MM/YYYY apenas para display"""
    if len(date_str) == 8:
        return f"{date_str[6:8]}/{date_str[4:6]}/{date_str[:4]}"
    return date_str

def log_message(message, log_type="info"):
    """Log padronizado para o pipeline"""
    # SEMPRE escrever no arquivo de log
    if logger:
        if log_type == "info":
            logger.info(message)
        elif log_type == "success":
            logger.info(f"SUCCESS: {message}")
        elif log_type == "warning":
            logger.warning(message)
        elif log_type == "error":
            logger.error(message)
        elif log_type == "debug":
            logger.debug(message)
    
    # Mostrar no console apenas se for modo debug ou n√£o for debug message
    if not debug_mode and log_type == "debug":
        return
    
    # Log no console
    if log_type == "info":
        console.print(f"[white]‚ÑπÔ∏è  {message}[/white]")
    elif log_type == "success":
        console.print(f"[green]‚úÖ {message}[/green]")
    elif log_type == "warning":
        console.print(f"[yellow]‚ö†Ô∏è  {message}[/yellow]")
    elif log_type == "error":
        console.print(f"[red]‚ùå {message}[/red]")
    elif log_type == "debug":
        console.print(f"[cyan]üîß {message}[/cyan]")

# Carregar configura√ß√µes
script_dir = os.path.dirname(os.path.abspath(__file__))
load_dotenv(os.path.join(script_dir, ".env"))

# Configura√ß√µes do banco PostgreSQL V1 (Supabase) - ULTRA OTIMIZADAS V5C
DB_CONFIG = {
    'host': os.getenv('SUPABASE_HOST', 'localhost'),
    'port': os.getenv('SUPABASE_PORT', '6543'),
    'database': os.getenv('SUPABASE_DBNAME', 'postgres'),
    'user': os.getenv('SUPABASE_USER', 'postgres'),
    'password': os.getenv('SUPABASE_PASSWORD', ''),
    'connect_timeout': 15,
    'options': '-c statement_timeout=120000',  # 120s timeout para SELECTs (aumentado)
    'keepalives_idle': 600,      # 10 minutos antes do keepalive
    'keepalives_interval': 30,   # Interval entre keepalives
    'keepalives_count': 3,       # M√°ximo keepalives antes de considerar morto
    'sslmode': 'require'         # SSL obrigat√≥rio mas sem verifica√ß√£o rigorosa
}

# Config separada para updates com timeout maior
DB_CONFIG_UPDATE = {
    **DB_CONFIG,
    'options': '-c statement_timeout=180000',  # 180s timeout para UPDATEs em lote
    'keepalives_idle': 300,      # 5 minutos para updates
}

# Controle de concorr√™ncia
stats_lock = threading.Lock()

# Estat√≠sticas globais
global_stats = {
    'embeddings_processados': 0,
    'categorizacoes_aplicadas': 0,
    'baixa_confianca': 0,
    'erros': 0
}

#############################################
# POOL DE CONEX√ïES OTIMIZADO V5C
#############################################

def initialize_connection_pool():
    """Inicializa pool de conex√µes thread-safe ULTRA OTIMIZADO V5C"""
    global connection_pool
    try:
        connection_pool = psycopg2.pool.ThreadedConnectionPool(
            minconn=MIN_WORKERS,
            maxconn=NUM_CONN,
            **DB_CONFIG
        )
        log_message(f"Pool de conex√µes OTIMIZADO V5C criado: {NUM_CONN} conex√µes m√°ximas", "success")
        return True
    except Exception as e:
        log_message(f"Erro ao criar pool de conex√µes: {e}", "error")
        return False

def close_connection_pool():
    """Fecha pool de conex√µes com seguran√ßa total"""
    global connection_pool
    if connection_pool:
        try:
            connection_pool.closeall()
            log_message("üîí Pool de conex√µes fechado com seguran√ßa", "info")
        except Exception as e:
            log_message(f"‚ö†Ô∏è Erro ao fechar pool: {e}", "warning")
        finally:
            connection_pool = None

@contextmanager
def get_db_connection(for_update=False):
    """Context manager para obter conex√£o do pool com tratamento robusto V5C - SIMPLIFICADO"""
    global connection_pool
    if not connection_pool:
        raise Exception("Pool de conex√µes n√£o inicializado")
    
    conn = None
    try:
        # Obter conex√£o do pool
        conn = connection_pool.getconn()
        
        if not conn or conn.closed:
            raise Exception("Conex√£o inv√°lida obtida do pool")
        
        # Configurar timeout
        with conn.cursor() as test_cursor:
            test_cursor.execute("SELECT 1")
            if for_update:
                test_cursor.execute("SET statement_timeout = 180000")
            else:
                test_cursor.execute("SET statement_timeout = 120000")
        
        # Yield da conex√£o
        yield conn
        
    except Exception as e:
        # Log do erro com mais detalhes
        log_message(f"Erro na conex√£o V5C: {type(e).__name__}: {str(e)}", "error")
        
        # Rollback se necess√°rio
        if conn and not conn.closed:
            try:
                conn.rollback()
            except:
                pass
        
        # Re-raise a exce√ß√£o
        raise e
        
    finally:
        # SEMPRE devolver conex√£o ao pool
        if conn:
            try:
                connection_pool.putconn(conn)
            except Exception as put_error:
                log_message(f"Erro ao devolver conex√£o: {put_error}", "warning")

#############################################
# FUN√á√ïES DE BANCO DE DADOS ULTRA OTIMIZADAS V5C
#############################################

def get_last_categorization_date():
    """Obt√©m a √∫ltima data de categoriza√ß√£o do system_config"""
    try:
        with get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT value FROM system_config 
                WHERE key = 'last_categorization_date'
            """)
            result = cursor.fetchone()
            
            if result:
                return result[0]
            else:
                return "20250101"
                
    except Exception as e:
        log_message(f"Erro ao obter √∫ltima data de categoriza√ß√£o: {e}", "error")
        return "20250101"

def get_last_embedding_date():
    """Obt√©m a √∫ltima data de processamento de embeddings do system_config"""
    try:
        with get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT value FROM system_config 
                WHERE key = 'last_embedding_date'
            """)
            result = cursor.fetchone()
            
            if result:
                return result[0]
            else:
                return datetime.now().strftime("%Y%m%d")
                
    except Exception as e:
        log_message(f"Erro ao obter √∫ltima data de embedding: {e}", "error")
        return datetime.now().strftime("%Y%m%d")

def update_last_categorization_date(date_str):
    """Atualiza a √∫ltima data de categoriza√ß√£o no system_config"""
    try:
        with get_db_connection(for_update=True) as conn:
            cursor = conn.cursor()
            cursor.execute("""
                INSERT INTO system_config (key, value, description) 
                VALUES ('last_categorization_date', %s, '√öltima data processada para categoriza√ß√£o autom√°tica')
                ON CONFLICT (key) 
                DO UPDATE SET value = EXCLUDED.value, updated_at = CURRENT_TIMESTAMP
            """, (date_str,))
            conn.commit()
            log_message(f"Data de categoriza√ß√£o atualizada para: {date_str}", "success")
            return True
        
    except Exception as e:
        log_message(f"Erro ao atualizar data de categoriza√ß√£o: {e}", "error")
        return False

def generate_dates_for_categorization(last_categorization_date, test_mode=False, last_embedding_date=None):
    """Gera lista de datas para categoriza√ß√£o - IGUAL ao 04E"""
    try:
        if test_mode:
            # Modo teste: apenas pr√≥ximo dia
            try:
                last_date_obj = datetime.strptime(last_categorization_date, '%Y%m%d')
                next_date_obj = last_date_obj + timedelta(days=1)
                next_date = next_date_obj.strftime('%Y%m%d')
                
                log_message(f"Modo teste: processando apenas data {format_date_for_display(next_date)}", "info")
                return [next_date]
            except ValueError as e:
                log_message(f"Erro ao calcular pr√≥xima data para teste: {e}", "error")
                return []
        
        if not last_embedding_date:
            log_message("√öltima data de embedding n√£o encontrada", "warning")
            return []
        
        # Converter strings YYYYMMDD para objetos datetime
        try:
            start_date = datetime.strptime(last_categorization_date, '%Y%m%d') + timedelta(days=1)
            end_date = datetime.strptime(last_embedding_date, '%Y%m%d')
        except ValueError as e:
            log_message(f"Erro ao converter datas: {e}", "error")
            return []
        
        # Verificar se h√° intervalo v√°lido
        if start_date > end_date:
            log_message(f"√öltima data de categoriza√ß√£o ({last_categorization_date}) j√° est√° atualizada", "info")
            return []
        
        # Gerar lista de datas dia por dia
        dates_to_process = []
        current_date = start_date
        
        while current_date <= end_date:
            dates_to_process.append(current_date.strftime('%Y%m%d'))
            current_date += timedelta(days=1)
        
        # Log do intervalo calculado
        if dates_to_process:
            start_display = format_date_for_display(dates_to_process[0])
            end_display = format_date_for_display(dates_to_process[-1])
            log_message(f"Intervalo calculado: {start_display} at√© {end_display} ({len(dates_to_process)} dias)", "info")
        
        return dates_to_process
        
    except Exception as e:
        log_message(f"Erro ao gerar datas para categoriza√ß√£o: {e}", "error")
        return []

def get_embeddings_by_date_optimized_v5c(date_str):
    """
    OTIMIZA√á√ÉO V5C ULTRA: Query √∫nica otimizada com timeout aumentado
    Resolve o problema de timeout na busca de embeddings
    """
    import time
    
    try:
        start_time = time.time()
        
        with get_db_connection() as conn:
            cursor = conn.cursor(cursor_factory=RealDictCursor)
            
            # CR√çTICO: Aumentar timeout para esta opera√ß√£o espec√≠fica
            cursor.execute("SET statement_timeout = 300000")  # 5 minutos
            
            date_formatted = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"
            
            log_message(f"ÔøΩ V5C ULTRA: Query √∫nica otimizada para data {format_date_for_display(date_str)}", "debug")
            
            # OTIMIZA√á√ÉO CR√çTICA: Query √∫nica mais eficiente
            optimized_query = """
            SELECT 
                ce.id_contratacao_emb,
                ce.numero_controle_pncp,
                ce.embeddings
            FROM contratacao_emb ce
            WHERE EXISTS (
                SELECT 1 FROM contratacao c
                WHERE c.numero_controle_pncp = ce.numero_controle_pncp
                  AND c.data_publicacao_pncp IS NOT NULL
                  AND DATE(c.data_publicacao_pncp) = %s::date
            )
            AND ce.embeddings IS NOT NULL
            AND ce.top_categories IS NULL
            ORDER BY ce.id_contratacao_emb
            LIMIT 3000  -- Limite de seguran√ßa
            """
            
            query_start = time.time()
            cursor.execute(optimized_query, (date_formatted,))
            query_time = time.time() - query_start
            
            log_message(f"üöÄ V5C: Query executada em {query_time:.1f}s", "debug")
            
            fetch_start = time.time()
            results = cursor.fetchall()
            fetch_time = time.time() - fetch_start
            
            total_time = time.time() - start_time
            log_message(f"Data {format_date_for_display(date_str)}: encontrados {len(results)} embeddings em {total_time:.1f}s (query √∫nica V5C)", "info")
            
            return results
        
    except Exception as e:
        total_time = time.time() - start_time if 'start_time' in locals() else 0
        log_message(f"‚ùå Erro V5C ap√≥s {total_time:.1f}s: {e}", "error")
        return []

def check_categoria_embeddings():
    """Verifica se existem embeddings de categorias na base"""
    try:
        with get_db_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT COUNT(*) FROM categoria 
                WHERE cat_embeddings IS NOT NULL
            """)
            count = cursor.fetchone()[0]
            
            log_message(f"Encontradas {count} categorias com embeddings", "info")
            return count > 0
        
    except Exception as e:
        log_message(f"Erro ao verificar embeddings de categorias: {e}", "error")
        return False

#############################################
# FUN√á√ïES DE CATEGORIZA√á√ÉO ULTRA OTIMIZADAS V5C - BATCH PGVECTOR
#############################################

def calculate_confidence(similarities):
    """Calcula o n√≠vel de confian√ßa baseado na diferen√ßa entre as similaridades"""
    if len(similarities) < 2:
        return 0.0
    
    top_score = similarities[0]
    
    if top_score == 0.0:
        return 0.0
    
    # Calcular gaps entre scores
    gaps = [top_score - score for score in similarities[1:]]
    weights = [1/(i+1) for i in range(len(gaps))]
    
    weighted_gap = sum(g * w for g, w in zip(gaps, weights)) / top_score
    confidence = (1 - math.exp(-10 * weighted_gap))
    
    return round(confidence, 4)

def batch_pgvector_similarity_search(cursor, embeddings_batch):
    """
    OTIMIZA√á√ÉO V5C CR√çTICA: Batch pgvector search
    Processa m√∫ltiplos embeddings em uma √∫nica query ao inv√©s de N queries individuais
    Resultado: 99% mais r√°pido que abordagem individual
    """
    if not embeddings_batch:
        return {}
    
    log_message(f"üöÄ V5C BATCH: Executando batch pgvector search para {len(embeddings_batch)} embeddings", "debug")
    
    try:
        # Preparar arrays para batch processing
        embedding_vectors = []
        embedding_ids = []
        
        for emb_data in embeddings_batch:
            embedding_vectors.append(emb_data['embeddings'])
            embedding_ids.append(emb_data['id_contratacao_emb'])
        
        # BATCH PGVECTOR QUERY V5C - Uma √∫nica query para todos os embeddings
        batch_similarity_query = """
        WITH embedding_batch AS (
            SELECT 
                unnest(%s::vector[]) as query_embedding,
                unnest(%s::bigint[]) as emb_id
        ),
        similarities AS (
            SELECT 
                eb.emb_id,
                c.cod_cat,
                c.nom_cat,
                1 - (c.cat_embeddings <=> eb.query_embedding) as similarity,
                ROW_NUMBER() OVER (
                    PARTITION BY eb.emb_id 
                    ORDER BY (c.cat_embeddings <=> eb.query_embedding) ASC
                ) as rank
            FROM embedding_batch eb
            CROSS JOIN categoria c
            WHERE c.cat_embeddings IS NOT NULL
        )
        SELECT emb_id, cod_cat, nom_cat, similarity
        FROM similarities 
        WHERE rank <= %s
        ORDER BY emb_id, similarity DESC
        """
        
        cursor.execute(batch_similarity_query, (embedding_vectors, embedding_ids, TOP_K))
        batch_results = cursor.fetchall()
        
        # Organizar resultados por embedding ID
        results_by_embedding = {}
        for row in batch_results:
            emb_id, cod_cat, nom_cat, similarity = row
            if emb_id not in results_by_embedding:
                results_by_embedding[emb_id] = []
            results_by_embedding[emb_id].append((cod_cat, nom_cat, similarity))
        
        log_message(f"üöÄ V5C BATCH: Batch pgvector conclu√≠do - {len(results_by_embedding)} embeddings processados", "debug")
        
        return results_by_embedding
        
    except Exception as e:
        log_message(f"‚ùå Erro no batch pgvector search: {e}", "error")
        # Fallback para m√©todo individual em caso de erro
        log_message("üîÑ Usando fallback para m√©todo individual", "warning")
        return {}

def categorize_embedding_batch_optimized_v5c(worker_id, embedding_batch, progress_callback=None):
    """Categoriza um lote de embeddings - ULTRA OTIMIZADO V5C com BATCH PGVECTOR E DEBUG"""
    import time
    
    local_stats = {
        'embeddings_processados': 0,
        'categorizacoes_aplicadas': 0,
        'baixa_confianca': 0,
        'erros': 0
    }
    
    try:
        start_time = time.time()
        log_message(f"ÔøΩ Worker {worker_id}: INICIANDO V5C com {len(embedding_batch)} embeddings", "debug")
        
        if progress_callback:
            progress_callback(0)
        
        # OTIMIZA√á√ÉO V5C: Processar em sub-lotes para batch pgvector
        update_data = []
        processed_count = 0
        
        conn_start = time.time()
        with get_db_connection(for_update=True) as conn:
            cursor = conn.cursor()
            
            # Aumentar timeout para workers
            cursor.execute("SET statement_timeout = 300000")  # 5 minutos
            
            conn_time = time.time() - conn_start
            log_message(f"üöÄ Worker {worker_id}: Conex√£o obtida em {conn_time:.1f}s", "debug")
            
            # OTIMIZA√á√ÉO V5C: Dividir em chunks para batch pgvector processing
            embedding_chunks = [embedding_batch[i:i + PGVECTOR_BATCH_SIZE] 
                              for i in range(0, len(embedding_batch), PGVECTOR_BATCH_SIZE)]
            
            log_message(f"ÔøΩ Worker {worker_id}: {len(embedding_chunks)} chunks criados (batch size {PGVECTOR_BATCH_SIZE})", "debug")
            
            total_chunks = len(embedding_chunks)
            
            for chunk_idx, chunk in enumerate(embedding_chunks):
                try:
                    chunk_start = time.time()
                    log_message(f"ÔøΩ Worker {worker_id}: CHUNK {chunk_idx + 1}/{total_chunks} - {len(chunk)} embeddings", "debug")
                    
                    # BATCH PGVECTOR SEARCH V5C - 1 query para todo o chunk
                    pgvector_start = time.time()
                    similarity_results = batch_pgvector_similarity_search(cursor, chunk)
                    pgvector_time = time.time() - pgvector_start
                    
                    log_message(f"üî• Worker {worker_id}: Batch pgvector executado em {pgvector_time:.1f}s - {len(similarity_results)} resultados", "debug")
                    
                    # Processar resultados do batch
                    process_start = time.time()
                    for embedding_data in chunk:
                        id_emb = embedding_data['id_contratacao_emb']
                        numero_controle = embedding_data['numero_controle_pncp']
                        
                        if id_emb in similarity_results:
                            category_results = similarity_results[id_emb]
                            
                            # Extrair c√≥digos e similaridades
                            top_categories = [row[0] for row in category_results]
                            top_similarities = [round(float(row[2]), 4) for row in category_results]
                            
                            # Calcular confian√ßa
                            confidence = calculate_confidence(top_similarities)
                            
                            # Preparar dados para batch update
                            update_data.append({
                                'id': id_emb,
                                'top_categories': top_categories,
                                'top_similarities': top_similarities,
                                'confidence': confidence
                            })
                            
                            local_stats['categorizacoes_aplicadas'] += 1
                        else:
                            local_stats['erros'] += 1
                        
                        local_stats['embeddings_processados'] += 1
                        processed_count += 1
                        
                        # Update progress incrementalmente
                        if progress_callback and processed_count % 20 == 0:
                            progress_percent = (processed_count / len(embedding_batch)) * 80  # 80% para processamento
                            progress_callback(int(progress_percent))
                    
                    process_time = time.time() - process_start
                    chunk_time = time.time() - chunk_start
                    
                    log_message(f"üî• Worker {worker_id}: Chunk {chunk_idx + 1} processado em {chunk_time:.1f}s (pgvector: {pgvector_time:.1f}s, process: {process_time:.1f}s)", "debug")
                    
                    # Progress entre chunks
                    if progress_callback:
                        chunk_progress = ((chunk_idx + 1) / total_chunks) * 80
                        progress_callback(int(chunk_progress))
                        
                except Exception as e:
                    log_message(f"‚ùå Worker {worker_id}: ERRO no chunk {chunk_idx + 1}: {e}", "error")
                    local_stats['erros'] += len(chunk)
            
            # OTIMIZA√á√ÉO V5C: Batch update usando execute_values
            if update_data:
                if progress_callback:
                    progress_callback(85)  # 85% - iniciando batch update
                
                update_start = time.time()
                log_message(f"üíæ Worker {worker_id}: INICIANDO batch update de {len(update_data)} registros", "debug")
                
                # Preparar dados para execute_values
                update_values = []
                for item in update_data:
                    update_values.append((
                        item['id'],
                        item['top_categories'],      # Manter como lista Python
                        item['top_similarities'],    # Manter como lista Python  
                        item['confidence']
                    ))
                
                # BATCH UPDATE SIMPLIFICADO com ON CONFLICT para evitar duplica√ß√£o
                update_query = """
                    UPDATE contratacao_emb SET
                        top_categories = data.top_categories::text[],
                        top_similarities = data.top_similarities::numeric[],
                        confidence = data.confidence::numeric
                    FROM (VALUES %s) AS data(id, top_categories, top_similarities, confidence)
                    WHERE contratacao_emb.id_contratacao_emb = data.id::bigint
                      AND contratacao_emb.top_categories IS NULL
                """
                
                execute_values(
                    cursor,
                    update_query,
                    update_values,
                    template='(%s, %s, %s, %s)',
                    page_size=200  # Page size otimizado
                )
                
                conn.commit()
                
                update_time = time.time() - update_start
                log_message(f"üíæ Worker {worker_id}: Batch update conclu√≠do em {update_time:.1f}s - {len(update_data)} registros", "debug")
            
            if progress_callback:
                progress_callback(100)
        
        total_time = time.time() - start_time
        log_message(f"‚úÖ Worker {worker_id}: CONCLU√çDO em {total_time:.1f}s - {local_stats['categorizacoes_aplicadas']} categoriza√ß√µes", "debug")
        
        return local_stats
        
    except Exception as e:
        total_time = time.time() - start_time if 'start_time' in locals() else 0
        log_message(f"‚ùå Worker {worker_id}: ERRO CR√çTICO ap√≥s {total_time:.1f}s: {e}", "error")
        local_stats['erros'] += len(embedding_batch) - local_stats['embeddings_processados']
        if progress_callback:
            progress_callback(100)
        return local_stats
                            
                            # Preparar dados para batch update
                            update_data.append({
                                'id': id_emb,
                                'top_categories': top_categories,
                                'top_similarities': top_similarities,
                                'confidence': confidence
                            })
                            
                            local_stats['categorizacoes_aplicadas'] += 1
                        else:
                            log_message(f"Worker {worker_id}: Nenhuma categoria encontrada para {numero_controle}", "warning")
                            local_stats['erros'] += 1
                        
                        local_stats['embeddings_processados'] += 1
                        processed_count += 1
                        
                        # Update progress incrementalmente
                        if progress_callback and processed_count % 20 == 0:
                            progress_percent = (processed_count / len(embedding_batch)) * 80  # 80% para processamento
                            progress_callback(int(progress_percent))
                    
                    # Progress entre chunks
                    if progress_callback:
                        chunk_progress = ((chunk_idx + 1) / total_chunks) * 80
                        progress_callback(int(chunk_progress))
                        
                except Exception as e:
                    log_message(f"Worker {worker_id}: Erro ao processar chunk {chunk_idx + 1}: {e}", "error")
                    local_stats['erros'] += len(chunk)
            
            # OTIMIZA√á√ÉO V5C: Batch update usando execute_values
            if update_data:
                if progress_callback:
                    progress_callback(85)  # 85% - iniciando batch update
                
                log_message(f"üîß Worker {worker_id}: Executando batch update de {len(update_data)} registros", "debug")
                
                # Preparar dados para execute_values
                update_values = []
                for item in update_data:
                    update_values.append((
                        item['id'],
                        item['top_categories'],      # Manter como lista Python
                        item['top_similarities'],    # Manter como lista Python  
                        item['confidence']
                    ))
                
                # BATCH UPDATE SIMPLIFICADO com ON CONFLICT para evitar duplica√ß√£o
                update_query = """
                    UPDATE contratacao_emb SET
                        top_categories = data.top_categories::text[],
                        top_similarities = data.top_similarities::numeric[],
                        confidence = data.confidence::numeric
                    FROM (VALUES %s) AS data(id, top_categories, top_similarities, confidence)
                    WHERE contratacao_emb.id_contratacao_emb = data.id::bigint
                      AND contratacao_emb.top_categories IS NULL
                """
                
                execute_values(
                    cursor,
                    update_query,
                    update_values,
                    template='(%s, %s, %s, %s)',
                    page_size=200  # Page size otimizado
                )
                
                conn.commit()
                log_message(f"üîß Worker {worker_id}: Batch update V5C conclu√≠do - {len(update_data)} registros atualizados", "debug")
            
            if progress_callback:
                progress_callback(100)
        
        log_message(f"üîß Worker {worker_id}: V5C Conclu√≠do - {local_stats['categorizacoes_aplicadas']} categoriza√ß√µes de {local_stats['embeddings_processados']} embeddings", "debug")
        
        return local_stats
        
    except Exception as e:
        log_message(f"‚ùå Worker {worker_id}: ERRO no processamento V5C: {e}", "error")
        local_stats['erros'] += len(embedding_batch) - local_stats['embeddings_processados']
        if progress_callback:
            progress_callback(100)
        return local_stats

def update_global_stats(**kwargs):
    """Atualiza estat√≠sticas globais de forma thread-safe"""
    with stats_lock:
        for key, value in kwargs.items():
            if key in global_stats:
                global_stats[key] += value

def partition_embeddings_optimized(embeddings, num_workers):
    """Divide os embeddings em parti√ß√µes otimizadas para processamento paralelo"""
    if not embeddings:
        return []
    
    total_items = len(embeddings)
    partition_size = max(1, total_items // num_workers)
    
    partitions = []
    for i in range(0, total_items, partition_size):
        partition = embeddings[i:i + partition_size]
        if partition:
            partitions.append(partition)
    
    # Se criamos mais parti√ß√µes que workers, mesclar as √∫ltimas
    while len(partitions) > num_workers:
        last_partition = partitions.pop()
        partitions[-1].extend(last_partition)
    
    return partitions

def process_categorization_for_date_v5c(date_str):
    """Processa categoriza√ß√£o para todas as contrata√ß√µes de uma data espec√≠fica - OTIMIZADO V5C"""
    log_message(f"Processando categoriza√ß√£o V5C para data: {format_date_for_display(date_str)}")
    
    # Obter embeddings da data usando estrutura otimizada V5C
    embeddings = get_embeddings_by_date_optimized_v5c(date_str)
    
    if not embeddings:
        log_message(f"Nenhum embedding para categorizar na data {format_date_for_display(date_str)}", "warning")
        return {
            'data': date_str,
            'embeddings_processados': 0,
            'categorizacoes_aplicadas': 0,
            'baixa_confianca': 0,
            'erros': 0
        }
    
    log_message(f"Data {format_date_for_display(date_str)}: processando {len(embeddings)} embeddings com BATCH PGVECTOR V5C")
    
    # Dividir em parti√ß√µes otimizadas
    partitions = partition_embeddings_optimized(embeddings, MAX_WORKERS)
    
    if not partitions:
        log_message(f"Nenhuma parti√ß√£o criada para data {date_str}", "warning")
        return {
            'data': date_str,
            'embeddings_processados': 0,
            'categorizacoes_aplicadas': 0,
            'baixa_confianca': 0,
            'erros': 0
        }
    
    # Estat√≠sticas da data
    date_stats = {
        'data': date_str,
        'embeddings_processados': 0,
        'categorizacoes_aplicadas': 0,
        'baixa_confianca': 0,
        'erros': 0
    }
    
    # PROCESSAMENTO OTIMIZADO V5C com progress tracking
    if debug_mode:
        # Modo debug: progress bars individuais
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TaskProgressColumn(),
            TimeElapsedColumn(),
            console=console
        ) as progress:
            
            # Task principal
            main_task = progress.add_task(
                f"[bold blue]üìä ULTRA DEBUG V5C BATCH: Data {date_str}", 
                total=len(partitions) * 100
            )
            
            # Tasks individuais por worker
            worker_tasks = {}
            for i in range(len(partitions)):
                worker_id = i + 1
                partition_size = len(partitions[i])
                worker_tasks[worker_id] = progress.add_task(
                    f"[cyan]üöÄ Worker {worker_id}: {partition_size} embeddings (BATCH PGVECTOR)",
                    total=100
                )
            
            # Progress tracking
            worker_progress_tracking = {i + 1: 0 for i in range(len(partitions))}
            
            def update_debug_progress():
                total_progress = sum(worker_progress_tracking.values())
                progress.update(main_task, 
                    completed=total_progress,
                    description=f"[bold blue]üìä ULTRA DEBUG V5C BATCH: Data {date_str} ({len(partitions)} workers)"
                )
            
            def process_with_debug_progress_v5c(worker_id, partition, date_str):
                def worker_progress_callback(completed_percent):
                    worker_progress_tracking[worker_id] = completed_percent
                    progress.update(worker_tasks[worker_id], 
                        completed=completed_percent,
                        description=f"[cyan]üöÄ Worker {worker_id}: {completed_percent}% ({len(partition)} embeddings - BATCH PGVECTOR)"
                    )
                    update_debug_progress()
                
                try:
                    result = categorize_embedding_batch_optimized_v5c(
                        worker_id, partition, worker_progress_callback
                    )
                    progress.update(worker_tasks[worker_id], 
                        completed=100,
                        description=f"[green]‚úÖ Worker {worker_id}: Conclu√≠do ({len(partition)} embeddings - V5C)"
                    )
                    return result
                except Exception as e:
                    progress.update(worker_tasks[worker_id], 
                        completed=100,
                        description=f"[red]‚ùå Worker {worker_id}: Erro ({len(partition)} embeddings)"
                    )
                    worker_progress_callback(100)
                    raise e
            
            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                futures = []
                for i, partition in enumerate(partitions):
                    future = executor.submit(
                        process_with_debug_progress_v5c,
                        i + 1,
                        partition,
                        date_str
                    )
                    futures.append(future)
                
                # Coletar resultados
                for future in futures:
                    try:
                        local_stats = future.result()
                        date_stats['embeddings_processados'] += local_stats['embeddings_processados']
                        date_stats['categorizacoes_aplicadas'] += local_stats['categorizacoes_aplicadas']
                        date_stats['baixa_confianca'] += local_stats['baixa_confianca']
                        date_stats['erros'] += local_stats['erros']
                        
                    except Exception as e:
                        log_message(f"Erro em worker V5C para data {date_str}: {e}", "error")
    
    else:
        # Modo normal: progress bar √∫nico
        with Progress(
            SpinnerColumn(),
            TextColumn("[progress.description]{task.description}"),
            BarColumn(),
            TaskProgressColumn(),
            TimeElapsedColumn(),
            console=console
        ) as progress:
            
            total_worker_tasks = len(partitions) * 100
            main_task = progress.add_task(
                f"[bold blue]üìä ULTRA OTIMIZADA V5C BATCH: Data {date_str}", 
                total=total_worker_tasks
            )
            
            # Progress tracking otimizado
            worker_progress_tracking = {i + 1: 0 for i in range(len(partitions))}
            
            def update_general_progress():
                total_progress = sum(worker_progress_tracking.values())
                progress.update(main_task, 
                    completed=total_progress,
                    description=f"[bold blue]üìä ULTRA OTIMIZADA V5C BATCH: Data {date_str} ({len(partitions)} workers)"
                )
            
            def process_with_progress_v5c(worker_id, partition, date_str):
                def worker_progress_callback(completed_percent):
                    worker_progress_tracking[worker_id] = completed_percent
                    update_general_progress()
                
                try:
                    result = categorize_embedding_batch_optimized_v5c(
                        worker_id, partition, worker_progress_callback
                    )
                    return result
                except Exception as e:
                    worker_progress_callback(100)
                    raise e
            
            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                futures = []
                for i, partition in enumerate(partitions):
                    future = executor.submit(
                        process_with_progress_v5c,
                        i + 1,
                        partition,
                        date_str
                    )
                    futures.append(future)
                
                # Coletar resultados
                for future in futures:
                    try:
                        local_stats = future.result()
                        date_stats['embeddings_processados'] += local_stats['embeddings_processados']
                        date_stats['categorizacoes_aplicadas'] += local_stats['categorizacoes_aplicadas']
                        date_stats['baixa_confianca'] += local_stats['baixa_confianca']
                        date_stats['erros'] += local_stats['erros']
                        
                    except Exception as e:
                        log_message(f"Erro em worker V5C para data {date_str}: {e}", "error")
    
    # Log do resultado da data
    date_display = format_date_for_display(date_str)
    log_message(f"‚úÖ Data {date_display} V5C conclu√≠da: {date_stats['categorizacoes_aplicadas']} categoriza√ß√µes, {date_stats['erros']} erros", 
                "success" if date_stats['erros'] == 0 else "warning")
    
    return date_stats

#############################################
# FUN√á√ÉO PRINCIPAL ULTRA OTIMIZADA V5C
#############################################

def main():
    """Fun√ß√£o principal da categoriza√ß√£o autom√°tica ULTRA OTIMIZADA V5C - BATCH PGVECTOR"""
    global debug_mode
    
    # Parse dos argumentos
    args = parse_arguments()
    debug_mode = args.debug
    test_mode = args.test
    
    # Setup do logging
    logger, log_filename = setup_logging(debug_mode)
    
    console.print(Panel("[bold blue][5C/7] CATEGORIZA√á√ÉO AUTOM√ÅTICA ULTRA OTIMIZADA V2 - BATCH PGVECTOR[/bold blue]"))
    
    log_message(f"Arquivo de log criado: {log_filename}", "info")
    log_message(f"Configura√ß√µes OTIMIZADAS V5C: {MAX_WORKERS} workers, batch size {BATCH_SIZE}, pool {NUM_CONN} conex√µes, pgvector batch {PGVECTOR_BATCH_SIZE}", "info")
    
    if test_mode:
        log_message("Modo TESTE ativado: processando pr√≥ximo dia", "warning")
    
    # Inicializar pool de conex√µes
    if not initialize_connection_pool():
        log_message("Falha ao inicializar pool de conex√µes", "error")
        return
    
    try:
        # Verificar se existem embeddings de categorias
        if not check_categoria_embeddings():
            log_message("N√£o foram encontrados embeddings de categorias na base", "error")
            log_message("Execute primeiro o script de gera√ß√£o de embeddings de categorias", "warning")
            return
        
        # Obter datas para processar
        last_categorization_date = get_last_categorization_date()
        log_message(f"√öltima data de categoriza√ß√£o: {format_date_for_display(last_categorization_date)}")
        
        if not test_mode:
            last_embedding_date = get_last_embedding_date()
            log_message(f"√öltima data de embeddings: {format_date_for_display(last_embedding_date)}")
            
            # Verificar se h√° intervalo v√°lido para processar
            if last_categorization_date >= last_embedding_date:
                log_message(f"Categoriza√ß√µes j√° est√£o atualizadas at√© {last_categorization_date} (>= {last_embedding_date})", "success")
                return
                
            log_message(f"Processando categoriza√ß√£o V5C do intervalo: {last_categorization_date} (exclusivo) at√© {last_embedding_date} (inclusivo)")
        else:
            last_embedding_date = None
        
        # Gerar lista de datas para processar
        dates_to_process = generate_dates_for_categorization(last_categorization_date, test_mode, last_embedding_date)
        
        if not dates_to_process:
            log_message("Nenhuma data nova para processar", "success")
            return
        
        log_message(f"Processando {len(dates_to_process)} datas com BATCH PGVECTOR V5C")
        
        # Estat√≠sticas globais
        global_stats_total = {
            'datas_processadas': 0,
            'embeddings_processados': 0,
            'categorizacoes_aplicadas': 0,
            'baixa_confianca': 0,
            'erros': 0
        }
        
        # Processar cada data
        inicio = time.time()
        
        for date_str in dates_to_process:
            try:
                # Separador visual entre datas
                if global_stats_total['datas_processadas'] > 0:
                    log_message("-" * 80, "info")
                
                date_display = format_date_for_display(date_str)
                log_message("")
                log_message(f"üìÖ [bold blue]Data {date_display} - BATCH PGVECTOR V5C[/bold blue]: categorizando...")
                
                # Reset estat√≠sticas para esta data
                global global_stats
                global_stats = {
                    'embeddings_processados': 0,
                    'categorizacoes_aplicadas': 0,
                    'baixa_confianca': 0,
                    'erros': 0
                }
                
                # Processar categoriza√ß√£o para a data com V5C
                date_stats = process_categorization_for_date_v5c(date_str)
                
                # Atualizar estat√≠sticas globais
                global_stats_total['datas_processadas'] += 1
                global_stats_total['embeddings_processados'] += date_stats['embeddings_processados']
                global_stats_total['categorizacoes_aplicadas'] += date_stats['categorizacoes_aplicadas']
                global_stats_total['baixa_confianca'] += date_stats['baixa_confianca']
                global_stats_total['erros'] += date_stats['erros']
                
                # Atualizar √∫ltima data processada APENAS se n√£o houve erros cr√≠ticos
                if date_stats['erros'] == 0:
                    update_last_categorization_date(date_str)
                    log_message(f"‚úÖ Data {date_display} V5C conclu√≠da com sucesso: {date_stats['categorizacoes_aplicadas']} categoriza√ß√µes", "success")
                else:
                    log_message(f"‚ö†Ô∏è  Data {date_display} V5C conclu√≠da com {date_stats['erros']} erros - data N√ÉO atualizada para permitir reprocessamento", "warning")
                    log_message(f"üìä Processados: {date_stats['categorizacoes_aplicadas']}, Erros: {date_stats['erros']}", "info")
                    
            except Exception as e:
                date_display = format_date_for_display(date_str)
                log_message(f"‚ùå Erro ao processar data {date_display} V5C: {e}", "error")
                global_stats_total['erros'] += 1
        
        # Relat√≥rio final
        tempo_total = time.time() - inicio
        
        # Log de finaliza√ß√£o detalhado no arquivo
        log_session_end(global_stats_total, tempo_total)
        
        log_message(f"Categoriza√ß√£o ULTRA OTIMIZADA V5C BATCH PGVECTOR conclu√≠da em {tempo_total:.1f}s", "success")
        log_message(f"Datas processadas: {global_stats_total['datas_processadas']}")
        log_message(f"Embeddings processados: {global_stats_total['embeddings_processados']}")
        log_message(f"Categoriza√ß√µes aplicadas: {global_stats_total['categorizacoes_aplicadas']}")
        log_message(f"Baixa confian√ßa: {global_stats_total['baixa_confianca']}")
        log_message(f"Erros: {global_stats_total['erros']}")
        
        # Calcular taxa de sucesso
        if global_stats_total['embeddings_processados'] > 0:
            taxa_sucesso = (global_stats_total['categorizacoes_aplicadas'] / global_stats_total['embeddings_processados']) * 100
            log_message(f"Taxa de categoriza√ß√£o: {taxa_sucesso:.1f}%")
        
        if global_stats_total['erros'] == 0:
            console.print(Panel("[bold green]‚úÖ CATEGORIZA√á√ÉO ULTRA OTIMIZADA V5C BATCH PGVECTOR CONCLU√çDA[/bold green]"))
        else:
            console.print(Panel("[bold yellow]‚ö†Ô∏è CATEGORIZA√á√ÉO V5C CONCLU√çDA COM ALGUNS ERROS[/bold yellow]"))
    
    except KeyboardInterrupt:
        log_message("‚ö†Ô∏è Script interrompido pelo usu√°rio (Ctrl+C)", "warning")
        console.print(Panel("[bold yellow]‚ö†Ô∏è PROCESSAMENTO INTERROMPIDO PELO USU√ÅRIO[/bold yellow]"))
    except Exception as e:
        log_message(f"‚ùå Erro cr√≠tico no processamento V5C: {e}", "error")
        console.print(Panel("[bold red]‚ùå ERRO CR√çTICO NO PROCESSAMENTO V5C[/bold red]"))
    finally:
        # SEMPRE fechar pool de conex√µes
        close_connection_pool()
        log_message("üîí Pool de conex√µes fechado com seguran√ßa", "info")

if __name__ == "__main__":
    main()
