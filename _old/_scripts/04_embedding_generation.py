# =======================================================================
# [4/7] GERA√á√ÉO DE EMBEDDINGS - VERS√ÉO V1
# =======================================================================
# Este script gera embeddings das contrata√ß√µes usando OpenAI API
# e os armazena na base Supabase v1.
# 
# Funcionalidades:
# - L√™ contrata√ß√µes n√£o processadas da base v1
# - Concatena objeto_compra + descri√ß√µes dos itens
# - Gera embeddings usando text-embedding-3-large (3072 dimens√µes)
# - Armazena na tabela contratacao_emb
# - Atualiza system_config com √∫ltima data processada
# 
# Resultado: Embeddings prontos para categoriza√ß√£o (script 05)
# =======================================================================

import os
import sys
import time
import json
import pickle
import threading
import argparse
import logging
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor
import psycopg2
from psycopg2.extras import RealDictCursor, execute_values
import numpy as np
from dotenv import load_dotenv
from rich.console import Console
from rich.progress import Progress, TextColumn, BarColumn, SpinnerColumn, TaskProgressColumn, TimeElapsedColumn
from rich.panel import Panel
from openai import OpenAI

# Configura√ß√£o global
debug_mode = False
logger = None

# Configure Rich console
console = Console()

def parse_arguments():
    """Parse argumentos da linha de comando"""
    parser = argparse.ArgumentParser(description='Gera√ß√£o de Embeddings v1')
    parser.add_argument('--test', 
                       help='Executar para data espec√≠fica (formato YYYYMMDD)',
                       type=str)
    parser.add_argument('--debug', 
                       action='store_true',
                       help='Ativar modo debug com logs detalhados')
    return parser.parse_args()

def setup_logging(debug_mode=False):
    """Configura sistema de logging igual ao script 03"""
    global logger
    
    # Gerar timestamp para o nome do arquivo de log
    start_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    LOG_FILE = os.path.join(script_dir, f"04_embeddings_{start_timestamp}.log")
    
    # Configurar logger principal
    logger = logging.getLogger('embedding_generation')
    logger.setLevel(logging.DEBUG)
    
    # Formatter para o arquivo de log
    file_formatter = logging.Formatter(
        '%(asctime)s | %(levelname)-8s | %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # Handler para arquivo (sempre ativo)
    file_handler = logging.FileHandler(LOG_FILE, encoding='utf-8')
    file_handler.setLevel(logging.DEBUG)
    file_handler.setFormatter(file_formatter)
    logger.addHandler(file_handler)
    
    # REDUZIR DEBUG EXCESSIVO DA OPENAI - configurar loggers espec√≠ficos para n√≠vel ERROR
    openai_loggers = [
        'openai',
        'httpx',
        'httpcore',
        'httpcore.http11',
        'httpcore.connection',
        'urllib3.connectionpool',
        'requests.packages.urllib3.connectionpool'
    ]
    
    for logger_name in openai_loggers:
        openai_logger = logging.getLogger(logger_name)
        openai_logger.setLevel(logging.ERROR)  # S√≥ mostra erros, n√£o debug
        openai_logger.propagate = False  # N√£o propaga para loggers pais
    
    logger.info("=" * 80)
    logger.info("NOVA SESS√ÉO DE GERA√á√ÉO DE EMBEDDINGS INICIADA")
    logger.info("=" * 80)
    logger.info(f"Hor√°rio de in√≠cio: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"Modo debug: {'Ativado' if debug_mode else 'Desativado'}")
    logger.info(f"Arquivo de log: {LOG_FILE}")
    logger.info("-" * 80)
    
    return logger, os.path.basename(LOG_FILE)

def log_session_end(stats, tempo_total):
    """Finaliza a sess√£o de log com resumo"""
    if logger:
        logger.info("-" * 80)
        logger.info("SESS√ÉO DE GERA√á√ÉO DE EMBEDDINGS FINALIZADA")
        logger.info(f"Hor√°rio de t√©rmino: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"Tempo total: {tempo_total:.1f}s")
        logger.info(f"Datas processadas: {stats['datas_processadas']}")
        logger.info(f"Contrata√ß√µes processadas: {stats['contratacoes_processadas']:,}")
        logger.info(f"Embeddings gerados: {stats['embeddings_gerados']:,}")
        logger.info(f"Embeddings em cache: {stats['embeddings_cache']:,}")
        logger.info(f"Erros encontrados: {stats['erros']:,}")
        logger.info("=" * 80)

def log_message(message, log_type="info"):
    """Log padronizado para o pipeline - sempre salva em arquivo"""
    # SEMPRE escrever no arquivo de log
    if logger:
        if log_type == "info":
            logger.info(message)
        elif log_type == "success":
            logger.info(f"SUCCESS: {message}")
        elif log_type == "warning":
            logger.warning(message)
        elif log_type == "error":
            logger.error(message)
        elif log_type == "debug":
            logger.debug(message)
    
    # Mostrar no console apenas se for modo debug ou n√£o for debug message
    if not debug_mode and log_type == "debug":
        return  # N√£o mostra mensagens debug se modo debug estiver desativado
    
    # Log no console
    if log_type == "info":
        console.print(f"[white]‚ÑπÔ∏è  {message}[/white]")
    elif log_type == "success":
        console.print(f"[green]‚úÖ {message}[/green]")
    elif log_type == "warning":
        console.print(f"[yellow]‚ö†Ô∏è  {message}[/yellow]")
    elif log_type == "error":
        console.print(f"[red]‚ùå {message}[/red]")
    elif log_type == "debug":
        console.print(f"[cyan]üîß {message}[/cyan]")

# Carregar configura√ß√µes
script_dir = os.path.dirname(os.path.abspath(__file__))
load_dotenv(os.path.join(script_dir, ".env"))

# Configura√ß√µes do banco PostgreSQL V1 (Supabase) - mesmo padr√£o do script 03
DB_CONFIG = {
    'host': os.getenv('SUPABASE_HOST', 'localhost'),
    'port': os.getenv('SUPABASE_PORT', '6543'),
    'database': os.getenv('SUPABASE_DBNAME', 'postgres'),
    'user': os.getenv('SUPABASE_USER', 'postgres'),
    'password': os.getenv('SUPABASE_PASSWORD', '')
}

# Configura√ß√µes OpenAI
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Configura√ß√µes de embedding
EMBEDDING_MODEL = "text-embedding-3-large"
EMBEDDING_DIM = 3072
MAX_WORKERS = 16  # Aumentando workers para compensar remo√ß√£o de sublotes
BATCH_SIZE = 50

# Cache e controle de concorr√™ncia
cache_lock = threading.Lock()
stats_lock = threading.Lock()

# Estat√≠sticas globais
global_stats = {
    'contratacoes_processadas': 0,
    'embeddings_gerados': 0,
    'embeddings_cache': 0,
    'erros': 0
}

#############################################
# FUN√á√ïES DE BANCO DE DADOS
#############################################

def create_connection():
    """Cria conex√£o com o banco PostgreSQL/Supabase - mesmo padr√£o do script 03"""
    try:
        connection = psycopg2.connect(**DB_CONFIG)
        connection.set_session(autocommit=False)
        return connection
    except Exception as e:
        log_message(f"Erro ao conectar ao banco: {e}", "error")
        return None

def create_embedding_constraints_if_not_exists(conn):
    """Cria constraints se n√£o existirem na tabela contratacao_emb"""
    cursor = conn.cursor()
    
    try:
        # Constraint para contratacao_emb - evitar duplicatas por numero_controle_pncp
        cursor.execute("""
            DO $$ BEGIN
                ALTER TABLE contratacao_emb ADD CONSTRAINT contratacao_emb_numero_controle_unique 
                UNIQUE (numero_controle_pncp);
            EXCEPTION
                WHEN duplicate_table THEN NULL;
            END $$;
        """)
        
        conn.commit()
        # Remover log debug para evitar spam nos workers
        
    except Exception as e:
        conn.rollback()
        log_message(f"Aviso ao criar constraint de embeddings: {str(e)}", "warning")

def get_last_embedding_date():
    """Obt√©m a √∫ltima data de processamento de embeddings do system_config"""
    connection = create_connection()
    if not connection:
        return None
    
    try:
        cursor = connection.cursor()
        cursor.execute("""
            SELECT value FROM system_config 
            WHERE key = 'last_embedding_date'
        """)
        result = cursor.fetchone()
        cursor.close()
        connection.close()
        
        if result:
            return result[0]
        else:
            # Se n√£o existir, criar com data bem antiga
            return "20200101"
            
    except Exception as e:
        log_message(f"Erro ao obter √∫ltima data de embedding: {e}", "error")
        connection.close()
        return "20200101"

def get_last_processed_date():
    """Obt√©m a √∫ltima data de processamento geral do system_config"""
    connection = create_connection()
    if not connection:
        return None
    
    try:
        cursor = connection.cursor()
        cursor.execute("""
            SELECT value FROM system_config 
            WHERE key = 'last_processed_date'
        """)
        result = cursor.fetchone()
        cursor.close()
        connection.close()
        
        if result:
            return result[0]
        else:
            # Se n√£o existir, usar data atual como fallback
            return datetime.now().strftime("%Y%m%d")
            
    except Exception as e:
        log_message(f"Erro ao obter √∫ltima data processada: {e}", "error")
        connection.close()
        return datetime.now().strftime("%Y%m%d")

def update_last_embedding_date(date_str):
    """Atualiza a √∫ltima data de processamento de embeddings no system_config"""
    connection = create_connection()
    if not connection:
        return False
    
    try:
        cursor = connection.cursor()
        cursor.execute("""
            INSERT INTO system_config (key, value, description) 
            VALUES ('last_embedding_date', %s, '√öltima data processada para gera√ß√£o de embeddings')
            ON CONFLICT (key) 
            DO UPDATE SET value = EXCLUDED.value, updated_at = CURRENT_TIMESTAMP
        """, (date_str,))
        connection.commit()
        cursor.close()
        connection.close()
        log_message(f"Data de embedding atualizada para: {date_str}", "success")
        return True
        
    except Exception as e:
        log_message(f"Erro ao atualizar data de embedding: {e}", "error")
        connection.close()
        return False

def get_dates_needing_embeddings(last_embedding_date, test_date=None, last_processed_date=None):
    """Obt√©m datas que precisam de processamento de embeddings"""
    connection = create_connection()
    if not connection:
        return []
    
    try:
        cursor = connection.cursor()
        
        if test_date:
            # Modo teste: verificar se a data tem contrata√ß√µes
            # Usar data_publicacao_pncp ao inv√©s de created_at
            query = """
                SELECT DISTINCT TO_DATE(c.data_publicacao_pncp, 'YYYY-MM-DD') as data_processamento
                FROM contratacao c
                WHERE c.data_publicacao_pncp IS NOT NULL 
                  AND TO_DATE(c.data_publicacao_pncp, 'YYYY-MM-DD') = %s::date
                ORDER BY data_processamento
            """
            # Converter YYYYMMDD para formato de data
            test_date_formatted = f"{test_date[:4]}-{test_date[4:6]}-{test_date[6:8]}"
            cursor.execute(query, (test_date_formatted,))
            
        else:
            # Modo normal: processar datas no intervalo [last_embedding_date + 1 dia, last_processed_date]
            if last_processed_date:
                query = """
                    SELECT DISTINCT TO_DATE(c.data_publicacao_pncp, 'YYYY-MM-DD') as data_processamento
                    FROM contratacao c
                    WHERE c.data_publicacao_pncp IS NOT NULL 
                      AND TO_DATE(c.data_publicacao_pncp, 'YYYY-MM-DD') > %s::date
                      AND TO_DATE(c.data_publicacao_pncp, 'YYYY-MM-DD') <= %s::date
                      AND EXISTS (
                          SELECT 1 FROM contratacao c2
                          WHERE TO_DATE(c2.data_publicacao_pncp, 'YYYY-MM-DD') = TO_DATE(c.data_publicacao_pncp, 'YYYY-MM-DD')
                            AND c2.numero_controle_pncp NOT IN (
                                SELECT DISTINCT numero_controle_pncp 
                                FROM contratacao_emb 
                                WHERE numero_controle_pncp IS NOT NULL
                            )
                      )
                    ORDER BY data_processamento
                """
                # Converter YYYYMMDD para formato de data  
                last_embedding_date_formatted = f"{last_embedding_date[:4]}-{last_embedding_date[4:6]}-{last_embedding_date[6:8]}"
                last_processed_date_formatted = f"{last_processed_date[:4]}-{last_processed_date[4:6]}-{last_processed_date[6:8]}"
                cursor.execute(query, (last_embedding_date_formatted, last_processed_date_formatted))
            else:
                # Fallback: modo antigo (processar datas ap√≥s last_embedding_date sem limite superior)
                query = """
                    SELECT DISTINCT TO_DATE(c.data_publicacao_pncp, 'YYYY-MM-DD') as data_processamento
                    FROM contratacao c
                    WHERE c.data_publicacao_pncp IS NOT NULL 
                      AND TO_DATE(c.data_publicacao_pncp, 'YYYY-MM-DD') > %s::date
                      AND EXISTS (
                          SELECT 1 FROM contratacao c2
                          WHERE TO_DATE(c2.data_publicacao_pncp, 'YYYY-MM-DD') = TO_DATE(c.data_publicacao_pncp, 'YYYY-MM-DD')
                            AND c2.numero_controle_pncp NOT IN (
                                SELECT DISTINCT numero_controle_pncp 
                                FROM contratacao_emb 
                                WHERE numero_controle_pncp IS NOT NULL
                            )
                      )
                    ORDER BY data_processamento
                """
                # Converter YYYYMMDD para formato de data  
                last_embedding_date_formatted = f"{last_embedding_date[:4]}-{last_embedding_date[4:6]}-{last_embedding_date[6:8]}"
                cursor.execute(query, (last_embedding_date_formatted,))
        
        dates = [row[0].strftime('%Y%m%d') for row in cursor.fetchall()]
        cursor.close()
        connection.close()
        
        return dates
        
    except Exception as e:
        log_message(f"Erro ao buscar datas para processamento: {e}", "error")
        connection.close()
        return []

def count_contratacoes_by_date(date_str):
    """Conta contrata√ß√µes que precisam de embedding para uma data espec√≠fica"""
    connection = create_connection()
    if not connection:
        return 0
    
    try:
        cursor = connection.cursor()
        
        # Converter YYYYMMDD para formato de data
        date_formatted = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"
        
        query = """
            SELECT COUNT(DISTINCT c.numero_controle_pncp)
            FROM contratacao c
            WHERE c.data_publicacao_pncp IS NOT NULL 
              AND TO_DATE(c.data_publicacao_pncp, 'YYYY-MM-DD') = %s::date
              AND c.numero_controle_pncp NOT IN (
                  SELECT DISTINCT numero_controle_pncp 
                  FROM contratacao_emb 
                  WHERE numero_controle_pncp IS NOT NULL
              )
        """
        
        cursor.execute(query, (date_formatted,))
        count = cursor.fetchone()[0]
        cursor.close()
        connection.close()
        
        return count
        
    except Exception as e:
        log_message(f"Erro ao contar contrata√ß√µes da data {date_str}: {e}", "error")
        connection.close()
        return 0

def get_contratacoes_by_date(date_str):
    """
    Obt√©m contrata√ß√µes de uma data espec√≠fica para processamento de embeddings
    Concatena: objeto_compra || ' :: ' || string_agg(descricao_item, ' :: ')
    """
    connection = create_connection()
    if not connection:
        return []
    
    try:
        cursor = connection.cursor(cursor_factory=RealDictCursor)
        
        # Converter YYYYMMDD para formato de data
        date_formatted = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"
        
        # Query que replica a view vw_contratacoes para uma data espec√≠fica usando data_publicacao_pncp
        query = """
        SELECT 
            c.numero_controle_pncp,
            c.objeto_compra || ' :: ' || COALESCE(string_agg(COALESCE(i.descricao_item, ''), ' :: '), '') AS descricao_completa
        FROM contratacao c
        LEFT JOIN item_contratacao i ON c.numero_controle_pncp = i.numero_controle_pncp
        WHERE c.data_publicacao_pncp IS NOT NULL 
          AND TO_DATE(c.data_publicacao_pncp, 'YYYY-MM-DD') = %s::date
          AND c.numero_controle_pncp NOT IN (
              SELECT DISTINCT numero_controle_pncp 
              FROM contratacao_emb 
              WHERE numero_controle_pncp IS NOT NULL
          )
        GROUP BY c.numero_controle_pncp, c.objeto_compra
        ORDER BY c.numero_controle_pncp
        """
        
        cursor.execute(query, (date_formatted,))
        results = cursor.fetchall()
        cursor.close()
        connection.close()
        
        return results
        
    except Exception as e:
        log_message(f"Erro ao buscar contrata√ß√µes da data {date_str}: {e}", "error")
        connection.close()
        return []

def process_embeddings_for_date(date_str):
    """Processa embeddings para todas as contrata√ß√µes de uma data espec√≠fica"""
    log_message(f"Processando embeddings para data: {date_str}")
    
    # Obter contrata√ß√µes da data
    contratacoes = get_contratacoes_by_date(date_str)
    
    if not contratacoes:
        log_message(f"Nenhuma contrata√ß√£o para processar na data {date_str}", "warning")
        return {
            'data': date_str,
            'contratacoes_processadas': 0,
            'embeddings_gerados': 0,
            'embeddings_cache': 0,
            'erros': 0
        }
    
    log_message(f"Data {date_str}: processando {len(contratacoes)} contrata√ß√µes")
    
    # Verificar embeddings existentes
    existing_embeddings = check_existing_embeddings()
    
    # Dividir em parti√ß√µes para processamento paralelo
    partitions = partition_contratacoes(contratacoes, MAX_WORKERS)
    
    if not partitions:
        log_message(f"Nenhuma parti√ß√£o criada para data {date_str}", "warning")
        return {
            'data': date_str,
            'contratacoes_processadas': 0,
            'embeddings_gerados': 0,
            'embeddings_cache': 0,
            'erros': 0
        }
    
    # Estat√≠sticas da data
    date_stats = {
        'data': date_str,
        'contratacoes_processadas': 0,
        'embeddings_gerados': 0,
        'embeddings_cache': 0,
        'erros': 0
    }
    
    # Processar com workers paralelos usando um √∫nico Progress
    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        TaskProgressColumn(),
        TimeElapsedColumn(),
        console=console
    ) as progress:
        
        # Progress bar geral destacado (azul) - baseado no total de tarefas dos workers
        total_worker_tasks = len(partitions) * 100  # Cada worker tem 100% de progresso
        main_task = progress.add_task(
            f"[bold blue]üìä GERAL: Data {date_str} - Iniciando workers...", 
            total=total_worker_tasks
        )
        
        # Tasks individuais por worker (inicialmente invis√≠veis - apenas em modo debug)
        worker_tasks = {}
        if debug_mode:
            for i, partition in enumerate(partitions):
                worker_tasks[i + 1] = progress.add_task(
                    f"[cyan]üîß Worker {i + 1}: Aguardando...",
                    total=100,  # 100% para cada worker
                    visible=False
                )
        
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            # Dicion√°rio para rastrear progresso atual de cada worker
            worker_progress_tracking = {i + 1: 0 for i in range(len(partitions))}
            
            def update_general_progress():
                """Atualiza o progress geral baseado na soma dos progressos dos workers"""
                total_progress = sum(worker_progress_tracking.values())
                # Descri√ß√£o simples quando n√£o est√° em debug mode
                if debug_mode:
                    progress.update(main_task, 
                        completed=total_progress,
                        description=f"[bold blue]üìä GERAL: Data {date_str} - {total_progress:.0f}/{total_worker_tasks} tarefas conclu√≠das"
                    )
                else:
                    # Progress simples - apenas nome da data e barra de progresso
                    progress.update(main_task, 
                        completed=total_progress,
                        description=f"[bold blue]üìä GERAL: Data {date_str}"
                    )
            
            # Fun√ß√£o wrapper que atualiza progress dinamicamente
            def process_with_progress(worker_id, partition, existing_embeddings, date_str):
                task_id = worker_tasks.get(worker_id) if debug_mode else None
                
                # Callback para atualizar progresso geral quando worker progride
                def worker_progress_callback(completed_percent):
                    worker_progress_tracking[worker_id] = completed_percent
                    update_general_progress()
                
                try:
                    # Mostrar worker progress apenas em debug mode
                    if debug_mode and task_id:
                        progress.update(task_id, visible=True)
                    
                    # Processar com atualiza√ß√£o de etapas
                    if debug_mode and task_id:
                        result = process_embedding_batch_for_date_with_progress(
                            worker_id, partition, existing_embeddings, date_str, 
                            progress, task_id, worker_progress_callback
                        )
                    else:
                        # Modo simplificado sem progress detalhado por worker
                        result = process_embedding_batch_for_date_simple(
                            worker_id, partition, existing_embeddings, date_str, worker_progress_callback
                        )
                    
                    return result
                    
                except Exception as e:
                    # Marcar como erro e completar
                    if debug_mode and task_id:
                        progress.update(task_id, 
                            description=f"[red]üîß Worker {worker_id}: ERRO - {str(e)[:30]}...",
                            completed=100
                        )
                    worker_progress_callback(100)  # Worker completou (com erro)
                    raise e
                finally:
                    # Aguardar um pouco antes de ocultar (apenas em debug mode)
                    if debug_mode and task_id:
                        time.sleep(0.5)
                        progress.update(task_id, visible=False)
            
            futures = []
            for i, partition in enumerate(partitions):
                future = executor.submit(
                    process_with_progress,
                    i + 1,
                    partition,
                    existing_embeddings,
                    date_str
                )
                futures.append(future)
            
            # Aguardar conclus√£o e coletar estat√≠sticas
            for future in futures:
                try:
                    local_stats = future.result()
                    date_stats['contratacoes_processadas'] += local_stats['contratacoes_processadas']
                    date_stats['embeddings_gerados'] += local_stats['embeddings_gerados']
                    date_stats['embeddings_cache'] += local_stats['embeddings_cache']
                    date_stats['erros'] += local_stats['erros']
                    
                except Exception as e:
                    log_message(f"Erro em worker para data {date_str}: {e}", "error")
                    # Erro j√° foi tratado no callback do worker
    
    # Log do resultado da data
    log_message(f"Data {date_str} conclu√≠da: {date_stats['embeddings_gerados']} embeddings gerados, {date_stats['erros']} erros", 
                "success" if date_stats['erros'] == 0 else "warning")
    
    return date_stats

#############################################
# FUN√á√ïES DE EMBEDDING
#############################################

def check_existing_embeddings():
    """Verifica embeddings j√° existentes para evitar duplicatas"""
    connection = create_connection()
    if not connection:
        return set()
    
    try:
        cursor = connection.cursor()
        cursor.execute("SELECT numero_controle_pncp FROM contratacao_emb WHERE numero_controle_pncp IS NOT NULL")
        existing = set(row[0] for row in cursor.fetchall())
        cursor.close()
        connection.close()
        
        log_message(f"Encontrados {len(existing)} embeddings existentes", "info")
        return existing
        
    except Exception as e:
        log_message(f"Erro ao verificar embeddings existentes: {e}", "error")
        connection.close()
        return set()

#############################################
# FUN√á√ïES DE EMBEDDING
#############################################

def generate_embeddings_batch(texts, max_retries=3):
    """Gera embeddings para um lote de textos usando OpenAI API"""
    if not texts:
        return []
    
    # Validar e limpar textos
    validated_texts = []
    for text in texts:
        if not isinstance(text, str):
            text = str(text)
        
        if not text.strip():
            text = "sem descri√ß√£o"
        
        # Limitar tamanho do texto
        if len(text) > 8000:
            text = text[:8000]
            
        validated_texts.append(text)
    
    # Gerar embeddings com retry
    for attempt in range(max_retries):
        try:
            response = client.embeddings.create(
                model=EMBEDDING_MODEL,
                input=validated_texts
            )
            
            embeddings = []
            dimensoes_corretas = 0
            dimensoes_incorretas = 0
            
            for item in response.data:
                emb = np.array(item.embedding, dtype=np.float32)
                
                # Verificar dimens√£o
                if emb.shape[0] == EMBEDDING_DIM:
                    dimensoes_corretas += 1
                else:
                    dimensoes_incorretas += 1
                    log_message(f"Embedding com dimens√£o incorreta: {emb.shape[0]} != {EMBEDDING_DIM}", "warning")
                    # Ajustar dimens√£o se necess√°rio
                    if len(emb) > EMBEDDING_DIM:
                        emb = emb[:EMBEDDING_DIM]
                    else:
                        padded = np.zeros(EMBEDDING_DIM, dtype=np.float32)
                        padded[:len(emb)] = emb
                        emb = padded
                
                embeddings.append(emb)
            
            # Log resumido sobre as dimens√µes (apenas se houver problemas)
            if dimensoes_incorretas > 0:
                log_message(f"Embeddings: {dimensoes_corretas} corretas ({EMBEDDING_DIM}D), {dimensoes_incorretas} ajustadas", "warning")
            
            return embeddings
            
        except Exception as e:
            if "rate limit" in str(e).lower() and attempt < max_retries - 1:
                wait_time = (2 ** attempt) + 1
                log_message(f"Rate limit atingido. Tentativa {attempt+1}/{max_retries}. Aguardando {wait_time}s", "warning")
                time.sleep(wait_time)
            else:
                log_message(f"Erro na API OpenAI: {e}", "error")
                raise
    
    return []

def update_global_stats(**kwargs):
    """Atualiza estat√≠sticas globais de forma thread-safe"""
    with stats_lock:
        for key, value in kwargs.items():
            if key in global_stats:
                global_stats[key] += value

#############################################
# PROCESSAMENTO PARALELO
#############################################

def process_embedding_batch_for_date_simple(worker_id, batch_contratacoes, existing_embeddings, date_str, progress_callback=None):
    """Processa um lote de contrata√ß√µes para gerar embeddings (modo simplificado sem progress detalhado)"""
    local_stats = {
        'contratacoes_processadas': 0,
        'embeddings_gerados': 0,
        'embeddings_cache': 0,
        'erros': 0
    }
    
    try:
        # Atualizar callback - iniciando
        if progress_callback:
            progress_callback(0)
        
        # Filtrar contrata√ß√µes que ainda n√£o t√™m embedding
        to_process = []
        cache_count = 0
        
        for contratacao in batch_contratacoes:
            numero_controle = contratacao['numero_controle_pncp']
            if numero_controle not in existing_embeddings:
                to_process.append(contratacao)
            else:
                cache_count += 1
        
        local_stats['embeddings_cache'] = cache_count
        
        if not to_process:
            if progress_callback:
                progress_callback(100)
            return local_stats
        
        # Atualizar callback - meio do processamento
        if progress_callback:
            progress_callback(50)
        
        # Processar todas as contrata√ß√µes do worker de uma vez
        texts = [item['descricao_completa'] for item in to_process]
        
        # Gerar embeddings
        embeddings = generate_embeddings_batch(texts)
        
        # Inserir no banco
        success_count = insert_embeddings_batch(to_process, embeddings, worker_id)
        
        local_stats['embeddings_gerados'] = success_count
        local_stats['contratacoes_processadas'] = len(to_process)
        
        # Atualizar callback - finalizado
        if progress_callback:
            progress_callback(100)
        
        return local_stats
        
    except Exception as e:
        log_message(f"Worker {worker_id}: Erro no processamento: {e}", "error")
        local_stats['erros'] = len(batch_contratacoes)
        if progress_callback:
            progress_callback(100)
        return local_stats

def process_embedding_batch_for_date_with_progress(worker_id, batch_contratacoes, existing_embeddings, date_str, progress, task_id, progress_callback=None):
    """Processa um lote de contrata√ß√µes para gerar embeddings com progress din√¢mico"""
    local_stats = {
        'contratacoes_processadas': 0,
        'embeddings_gerados': 0,
        'embeddings_cache': 0,
        'erros': 0
    }
    
    def update_progress(completed, description):
        """Helper para atualizar progress do worker e callback geral"""
        progress.update(task_id, description=description, completed=completed)
        if progress_callback:
            progress_callback(completed)
    
    try:
        # ETAPA 1: Filtrar contrata√ß√µes (0-25%)
        update_progress(0, f"[cyan]üîß Worker {worker_id}: Filtrando {len(batch_contratacoes)} contrata√ß√µes...")
        
        to_process = []
        cache_count = 0
        
        for i, contratacao in enumerate(batch_contratacoes):
            numero_controle = contratacao['numero_controle_pncp']
            if numero_controle not in existing_embeddings:
                to_process.append(contratacao)
            else:
                cache_count += 1
            
            # Atualizar progress da filtragem
            current_progress = int((i + 1) / len(batch_contratacoes) * 25)
            update_progress(current_progress, f"[cyan]üîß Worker {worker_id}: Filtrando {len(batch_contratacoes)} contrata√ß√µes...")
        
        local_stats['embeddings_cache'] = cache_count
        
        if not to_process:
            update_progress(100, f"[green]üîß Worker {worker_id}: Conclu√≠do - {cache_count} j√° existiam")
            return local_stats
        
        # ETAPA 2: Preparar textos (25-50%)
        update_progress(25, f"[cyan]üîß Worker {worker_id}: Preparando {len(to_process)} textos...")
        
        texts = [item['descricao_completa'] for item in to_process]
        update_progress(50, f"[cyan]üîß Worker {worker_id}: Textos preparados")
        
        # ETAPA 3: Gerar embeddings (50-75%)
        update_progress(50, f"[cyan]üîß Worker {worker_id}: Gerando embeddings para {len(texts)} textos...")
        
        embeddings = generate_embeddings_batch(texts)
        update_progress(75, f"[cyan]üîß Worker {worker_id}: Embeddings gerados")
        
        # ETAPA 4: Inserir no banco (75-100%)
        update_progress(75, f"[cyan]üîß Worker {worker_id}: Inserindo {len(embeddings)} embeddings no banco...")
        
        success_count = insert_embeddings_batch(to_process, embeddings, worker_id)
        
        local_stats['embeddings_gerados'] = success_count
        local_stats['contratacoes_processadas'] = len(to_process)
        
        # FINALIZA√á√ÉO
        update_progress(100, f"[green]üîß Worker {worker_id}: Conclu√≠do - {success_count} embeddings inseridos")
        
        return local_stats
        
    except Exception as e:
        log_message(f"Worker {worker_id}: Erro no processamento: {e}", "error")
        update_progress(100, f"[red]üîß Worker {worker_id}: ERRO - {str(e)[:40]}...")
        local_stats['erros'] = len(batch_contratacoes)
        return local_stats

def process_embedding_batch_for_date(worker_id, batch_contratacoes, existing_embeddings, date_str):
    """Processa um lote de contrata√ß√µes para gerar embeddings (vers√£o simplificada)"""
    local_stats = {
        'contratacoes_processadas': 0,
        'embeddings_gerados': 0,
        'embeddings_cache': 0,
        'erros': 0
    }
    
    try:
        # Filtrar contrata√ß√µes que ainda n√£o t√™m embedding
        to_process = []
        cache_count = 0
        
        for contratacao in batch_contratacoes:
            numero_controle = contratacao['numero_controle_pncp']
            if numero_controle not in existing_embeddings:
                to_process.append(contratacao)
            else:
                cache_count += 1
        
        local_stats['embeddings_cache'] = cache_count
        
        if not to_process:
            return local_stats
        
        # Processar todas as contrata√ß√µes do worker de uma vez (sem sublotes)
        texts = [item['descricao_completa'] for item in to_process]
        
        # Gerar embeddings
        embeddings = generate_embeddings_batch(texts)
        
        # Inserir no banco
        success_count = insert_embeddings_batch(to_process, embeddings, worker_id)
        
        local_stats['embeddings_gerados'] = success_count
        local_stats['contratacoes_processadas'] = len(to_process)
        
        return local_stats
        
    except Exception as e:
        log_message(f"Worker {worker_id}: Erro no processamento: {e}", "error")
        local_stats['erros'] = len(batch_contratacoes)
        return local_stats

def partition_contratacoes(contratacoes, num_workers):
    """Divide as contrata√ß√µes em parti√ß√µes para processamento paralelo"""
    if not contratacoes:
        return []
    
    # Com mais workers, cada um processa menos dados de uma vez
    partition_size = max(1, len(contratacoes) // num_workers)
    
    partitions = []
    for i in range(0, len(contratacoes), partition_size):
        partition = contratacoes[i:i + partition_size]
        if partition:
            partitions.append(partition)
    
    return partitions

def insert_embeddings_batch(contratacoes, embeddings, worker_id):
    """Insere um lote de embeddings no banco de dados com verifica√ß√£o de duplicatas"""
    if len(contratacoes) != len(embeddings):
        log_message(f"Worker {worker_id}: Mismatch entre contrata√ß√µes e embeddings", "error")
        return 0
    
    connection = create_connection()
    if not connection:
        return 0
    
    try:
        # Garantir que constraint existe antes da inser√ß√£o
        create_embedding_constraints_if_not_exists(connection)
        
        cursor = connection.cursor()
        
        # Preparar dados para inser√ß√£o
        insert_data = []
        for contratacao, embedding in zip(contratacoes, embeddings):
            metadata = {
                "modelo": EMBEDDING_MODEL,
                "dimensoes": EMBEDDING_DIM,
                "timestamp": datetime.now().isoformat(),
                "worker_id": worker_id
            }
            
            insert_data.append((
                contratacao['numero_controle_pncp'],
                EMBEDDING_MODEL,
                json.dumps(metadata),
                embedding.tolist()  # Converter para lista para PostgreSQL
            ))
        
        # Inser√ß√£o em lote com ON CONFLICT e contagem precisa de inser√ß√µes reais
        insert_query = """
            WITH inserted AS (
                INSERT INTO contratacao_emb (
                    numero_controle_pncp, modelo_embedding, metadata, embeddings
                ) VALUES %s
                ON CONFLICT (numero_controle_pncp) DO NOTHING
                RETURNING numero_controle_pncp
            )
            SELECT COUNT(*) FROM inserted
        """
        
        # Executar inser√ß√£o e obter contagem real de registros inseridos
        execute_values(
            cursor,
            insert_query,
            insert_data,
            template='(%s, %s, %s, %s::vector)',
            page_size=len(insert_data),
            fetch=True
        )
        
        # Obter o resultado da contagem
        result = cursor.fetchone()
        inserted_count = result[0] if result else 0
        
        connection.commit()
        cursor.close()
        connection.close()
        
        return inserted_count
        
    except Exception as e:
        log_message(f"Worker {worker_id}: Erro na inser√ß√£o: {e}", "error")
        connection.rollback()
        connection.close()
        return 0

#############################################
# FUN√á√ÉO PRINCIPAL
#############################################

def main():
    """Fun√ß√£o principal do processamento de embeddings"""
    global debug_mode
    
    # Parse dos argumentos
    args = parse_arguments()
    debug_mode = args.debug
    test_date = args.test
    
    # Setup do logging
    logger, log_filename = setup_logging(debug_mode)
    
    console.print(Panel("[bold blue][4/7] GERA√á√ÉO DE EMBEDDINGS V1[/bold blue]"))
    
    # Log inicial informando o arquivo de log que est√° sendo usado
    log_message(f"Arquivo de log criado: {log_filename}", "info")
    
    # Verificar configura√ß√µes essenciais
    if not os.getenv("OPENAI_API_KEY"):
        log_message("OPENAI_API_KEY n√£o encontrada", "error")
        return
    
    # Verificar/criar constraints de unicidade para contratacao_emb
    test_conn = create_connection()
    if test_conn:
        create_embedding_constraints_if_not_exists(test_conn)
        test_conn.close()
        log_message("Constraint de unicidade verificada para contratacao_emb", "success")
    else:
        log_message("Falha ao verificar constraints do banco", "error")
        return
    
    if test_date:
        log_message(f"Modo TESTE ativado para data: {test_date}", "warning")
    
    # Obter √∫ltima data de processamento de embeddings
    last_embedding_date = get_last_embedding_date()
    log_message(f"√öltima data de embedding processada: {last_embedding_date}")
    
    # No modo produ√ß√£o, obter tamb√©m a √∫ltima data processada geral
    last_processed_date = None
    if not test_date:
        last_processed_date = get_last_processed_date()
        log_message(f"√öltima data processada geral: {last_processed_date}")
        
        # Verificar se h√° intervalo v√°lido para processar
        if last_embedding_date >= last_processed_date:
            log_message(f"Embeddings j√° est√£o atualizados at√© {last_embedding_date} (>= {last_processed_date})", "success")
            return
        
        log_message(f"Processando embeddings do intervalo: {last_embedding_date} (exclusivo) at√© {last_processed_date} (inclusivo)")
    
    # Buscar datas para processar
    dates_to_process = get_dates_needing_embeddings(last_embedding_date, test_date, last_processed_date)
    
    if not dates_to_process:
        log_message("Nenhuma data nova para processar", "success")
        return
    
    log_message(f"Processando {len(dates_to_process)} datas")
    
    # Estat√≠sticas globais
    global_stats = {
        'datas_processadas': 0,
        'contratacoes_processadas': 0,
        'embeddings_gerados': 0,
        'embeddings_cache': 0,
        'erros': 0
    }
    
    # Processar cada data
    inicio = time.time()
    
    for date_str in dates_to_process:
        try:
            # Contar contrata√ß√µes apenas da data atual (quando necess√°rio)
            count = count_contratacoes_by_date(date_str)
            log_message(f"Data {date_str}: {count} contrata√ß√µes para processar")
            
            # Processar embeddings para a data
            date_stats = process_embeddings_for_date(date_str)
            
            # Atualizar estat√≠sticas globais
            global_stats['datas_processadas'] += 1
            global_stats['contratacoes_processadas'] += date_stats['contratacoes_processadas']
            global_stats['embeddings_gerados'] += date_stats['embeddings_gerados']
            global_stats['embeddings_cache'] += date_stats['embeddings_cache']
            global_stats['erros'] += date_stats['erros']
            
            # Atualizar √∫ltima data processada (se n√£o for teste)
            if not test_date:
                update_last_embedding_date(date_str)
                
        except Exception as e:
            log_message(f"Erro ao processar data {date_str}: {e}", "error")
            global_stats['erros'] += count_contratacoes_by_date(date_str)
    
    # Relat√≥rio final
    tempo_total = time.time() - inicio
    
    # Log de finaliza√ß√£o detalhado no arquivo
    log_session_end(global_stats, tempo_total)
    
    log_message(f"Processamento conclu√≠do em {tempo_total:.1f}s", "success")
    log_message(f"Datas processadas: {global_stats['datas_processadas']}")
    log_message(f"Contrata√ß√µes processadas: {global_stats['contratacoes_processadas']}")
    log_message(f"Embeddings gerados: {global_stats['embeddings_gerados']}")
    log_message(f"Embeddings em cache: {global_stats['embeddings_cache']}")
    log_message(f"Erros: {global_stats['erros']}")
    
    if global_stats['erros'] == 0:
        console.print(Panel("[bold green]‚úÖ PROCESSAMENTO DE EMBEDDINGS CONCLU√çDO[/bold green]"))
    else:
        console.print(Panel("[bold yellow]‚ö†Ô∏è PROCESSAMENTO CONCLU√çDO COM ALGUNS ERROS[/bold yellow]"))

if __name__ == "__main__":
    main()
